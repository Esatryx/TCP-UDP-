# 3-数据采集1
## 数据的重要性
* 数据是构建 AI 系统所必需的关键基础设施。因为数据在很大程度上决定了 AI 系统的性能、公平性、稳健性、安全性和可扩展性

## 数据采集的挑战 
* 传统数据源对数据采集支持较差：
  * 业务数据库是为了支撑业务，数据表关联复杂，且一般针对高并发及低延迟的小操作进行设计
  * Web日志往往是为了方便业务调试来做。
* 数据团队与业务团队配合困难：
  * 数据分析让路产品升级；
  * 对于业务团队而言，数据采集属于额外工作

## 数据采集遵循法则 
* 海量数据，包括业务的各个角度
* 多种数据源，全量数据，全面覆盖
* 按需求将不同维度的数据都进行采集。
* 强调时效性，实时采集保障分析价值

## 埋点(用户行为数据采集)
* 埋点：在正常的业务逻辑中，嵌入数据采集的代码
* 埋点优势：数据是手动编码产生的，易于收集，灵活性大，扩展性强
* 埋点劣势：必须十分清楚目标，即需要收集什么样的数据必须提前确定；容易发生漏埋现象；产品迭代过程中，忽略了埋点逻辑的更改
* 埋点方式有哪些？
  * 全埋点/无埋点：“全部采集，按需选取”；在产品中嵌入SDK，做个统一埋点，一般用于采集APP的用户行为
  * 可视化埋点：在全埋点部署成功、可以获得全量数据的基础上，以可视化的方式，在对应页面上定义想要的页面数据，或者控件数据
  * 代码埋点：前端代码埋点和后端代码埋点。更适合精细化分析的场景，采集各种细粒度数据
  * 前两种方式是无埋码实现数据采集，比较适合给运营和市场人员使用；第三种需要技术人员通过编码来实施

* 无埋点优势：技术门槛低，部署使用简单；用户友好性强；收集的是全量数据，不会出现漏埋、误埋等现象
* 无埋点劣势：适用通用场景、标准化采集，自定义属性采集覆盖不了；采集全量数据，给数据传输和服务器增加压力；兼容性有限等
* 埋点采集数据的过程

<img width="730" alt="73709b3fd4fc62440b44eca8f25566f" src="https://github.com/user-attachments/assets/59ef9cb6-e98e-4cb7-a5e4-bebd754a94ed">

* 埋点方案应具备四个要素：
  * 确认事件与变量：事件指产品中的操作，变量指描述事件的属性。按照产品流程来设计关键事件
  * 明确事件的触发时机：不同触发时机代表不同的数据统计口径，要尽量选择最贴近业务的统计口径，然后再与开发沟通
  * 规范命名：对事件进行规范统一的命名，有助于提高数据的实用性及数据管理效率
  * 明确优先级：在设计埋点方案时，一定要对埋点事件有明确的优先级排布

## ETL(Extract-Transform-Load) (系统业务数据整合)
* ETL：用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程
* ETL是将业务系统的数据经过抽取、清洗转换之后加载到数据仓库的过程，目的是将企业中的分散、零乱、标准不统一的数据整合到一起，为企业的决策提供分析依据。是BI的重要一环，在BI项目中占至少三分之一的时间

<img width="776" alt="138c74b21bc9fcb77c2e6590886896e" src="https://github.com/user-attachments/assets/b17e83e3-13e9-4317-bf9b-c7ea0e4643d5">

* ETL的设计分为三部分：数据的抽取、数据的清洗转换、数据的加载。花费时间最长的是Transform（清洗转换）部分，一般情况下是整个ETL的2/3
* ETL常用的三种实现方法：借助ETL工具；SQL方式实现；ETL工具和SQL相结合
* ETL工具解决的问题：数据来自不同物理主机、数据来自不同数据库或者文件、异构数据处理等

### ETL——数据抽取
<img width="753" alt="1a60cf667388c267dace267960c323a" src="https://github.com/user-attachments/assets/c080564e-a57d-47fd-89e3-91433be9b724">

### ETL——数据转换
<img width="773" alt="19be13247428d5047191ad493df4bf8" src="https://github.com/user-attachments/assets/090c02f9-5904-49de-ab97-8d67addcd254">


### ETL——数据加载
<img width="757" alt="6bac7688a3f5d9b3fb7d3809afa1372" src="https://github.com/user-attachments/assets/3b21cf80-1e6b-451b-bc62-46a7f7938d06">

### 常用的ETL工具
* Kettle
* Apatar
* Scriptella
* ETL Automation

## 网络爬虫(互联网数据采集)
* 网络爬虫：是一种按照一定的规则，自动抓取万维网信息（网页）的程序或者脚本。为搜索引擎从万维网上抓取网页，是搜索引擎的重要组成部分
* 网络爬虫可分为通用网络爬虫和聚焦网络爬虫

<img width="758" alt="80bef729b558979181d113d7bf3574c" src="https://github.com/user-attachments/assets/3de3e55a-d4e5-49d2-8f95-2a4cb2233906">

<img width="762" alt="35fca274f6f609a7b8ba4e974b4f7d2" src="https://github.com/user-attachments/assets/ed34165a-3556-468e-aec3-acacbaf8de67">

* 网络爬虫视角下的网页：

<img width="707" alt="d163df8cc73b28de539db47a6d84fde" src="https://github.com/user-attachments/assets/a3e42158-b7af-4149-94cb-64924154ba02">

* 网络爬虫的抓取策略：
  * 深度优先遍历策略：从起始页开始，一个个链接跟踪下去。
  * 宽度优先遍历策略：抓取当前网页中链接的所有网页，再从待抓取队列中选取下一个URL
  * 反向链接数策略：反向链接数是指一个网页被其他网页链接指向的数量。使用这个指标评价网页的重要程度，从而决定抓取先后顺序
  * 基于优先级计算的策略：针对待抓取网页计算优先级值，通过排序来确定抓取顺序。如：Partial PageRank，OPIC等
  * 大站优先策略：对于待抓取队列中的所有网页，根据所属的网站进行分类，对于待下载页面数多的网站优先下载

* 定期更新策略
  * 历史参考策略：在网页的历史更新数据基础上，利用建模等手段，预测网页下一次更新的时间，确定爬取周期
  * 用户体验策略：依据网页多个历史版本的内容更新、搜索质量影响、用户体验等信息，来确定爬取周期
  * 聚类分析策略：首先对海量的网页进行聚类分析，每个类中的网页一般具有类似的更新频率。通过抽样计算，确定针对每个聚类的爬行频率

* 网络爬虫系统架构：往往是一个分布式系统。

<img width="773" alt="cc8249caeddd5e1afea9e8caee013ca" src="https://github.com/user-attachments/assets/60030490-8d48-4866-9141-11e0ed2f6696">

<img width="777" alt="aca30d90fd6b721bc9090518b38982f" src="https://github.com/user-attachments/assets/fce6adb4-db88-4426-b56d-d1dec07187d7">

<img width="802" alt="136c6c96b622180a1ac004082d8d080" src="https://github.com/user-attachments/assets/60f9af3b-0a6d-4176-ba4b-cded91cd1d65">
