# 5-分布式存储
## 大数据存储
* 重要手段：超大规模分布式文件系统，即以文件系统的方式来组织海量数据
* 代表工作：Google的文件系统——GFS

## Google文件系统(GFS )
* GFS 提供了海量非结构化信息的存储平台，并提供数据的冗余备份、成千台服务器的自动负载均衡以及失效服务器检测等各种完备的分布式存储功能
* 在GFS基础功能之上，可以开发更加符合应用需求的存储系统和计算框架

## 文件系统
* 文件系统是操作系统用于明确存储设备或分区上的文件的方法和数据结构，即在存储设备上组织文件的方法

## GFS设计原则 
* GFS采用大量商业PC来构建存储集群，PC的稳定性并没有很高的保障。部件错误不再被当作异常，而是将其作为常见的情况加以处理。数据冗余备份、机器有效性检测、机器故障恢复都被放在设计目标里
* GFS文件系统所存储的文件绝大多数都是大文件（GB），设计时针对大文件的读写操作进行了优化，尽管也支持小文件，但不作为重点
* 系统中存在大量的“追加写” 操作。即将新增内容追加到已有文件末尾，已经写入的内容一般不做更改，很少有文件的"随机写"行为
* 对于数据读取操作来说，绝大多数读文件操作都是“顺序读”，少量操作是“随机读”。往往是一次读取较大量的数据，而不是不断定位到某个位置读取少量数据

## GFS整体架构
* GFS文件系统主要由3个组成部分构成：唯一的主控服务器(Master)、众多的Chunk服务器和GFS客户端
  * 主控服务器：负责整体系统的管理工作
  * Chunk服务器：负责实际的数据存储并响应客户端的读/写请求
  * GFS客户端：用户操作

* GFS由上千台PC构成，但是在应用开发者的眼中，GFS类似于本地的统一文件系统，分布式存储系统的细节是隐藏的
* GFS文件存储：对于开发者而言，GFS类似于Linux文件系统或者是 Windows操作系统提供的文件系统，由目录和存放在目录下的文件构成树形结构，这个树形结构被称为GFS命名空间。GFS向应用开发者提供了文件的创建、删除、读取和写入等常见的操作接口(API)

<img width="422" alt="786ad57f1916cfb78dfe3f54ea13fc7" src="https://github.com/user-attachments/assets/169d9132-3e10-413c-9432-d338c7a74724">

* GFS命名空间由众多的目录和GFS文件构成， 一个GFS文件由众多固定大小的Chunk构成。而每个Chunk又由更小的粒度Block 构成，Chunk是基本存储单元，Block是基本读取单元

<img width="414" alt="ef18ad7a23e6775f9e61affb082d830" src="https://github.com/user-attachments/assets/534a543f-ab5c-45ce-bb0a-3160abac8d73">

## GFS主控服务器 
* GFS主控服务器所管理的系统元数据包括以下三种：
  * GFS命名空间和Chunk命名空间：主要用来对目录文件以及Chunk的增删改等信息进行记录
  * 从文件到其所属Chunk之间的映射关系：因为一个文件会被切割成众多Chunk，所以系统需要维护这种映射关系
  * Chunk在Chunk服务器上的存储信息：每个Chunk会被复制多个备份，并存储在不同的服务器上

<img width="428" alt="7c0f0d7b9cbc8cfa4d05069cce768d8" src="https://github.com/user-attachments/assets/293abf09-9676-4f5c-91c0-31cbe4714d2b">

* 主控服务器上管理数据的安全性需要保障：
  * GFS将前两类管理信息(命名空间，文件到Chunk的映射) 记录在系统日志文件内。并且将系统日志分别存储在多台机器上， 避免信息丢失。
  * 第3类管理信息(Chunk存储信息)通过主控服务器定期询问Chunk服务器来维护的

* 主控服务器所承担的系统管理工作：
  * 不同Chunk服务器之间的负载均衡。创建新的Chunk以及垃圾回收
  * 数据备份及迁移时需要考虑
    * Chunk数据的可用性；当Chunk数据不可用时，及时重新备份
    * 尽可能减少网络传输压力

* GFS是典型的主从结构方式。通过主控服务器实现全局管控，管理便捷
  * 缺点：系统瓶颈，单点失效
* GFS采用增加另外一台影子服务器(Shadow)的方式来解决单点失效问题。当主控服务器出现故障，无法提供服务时。由影子服务器接替主控服务器行使对应的管理功能

## GFS写操作示例
* GFS客户端发出写操作请求，GFS系统必须将这个写操作应用到 Chunk的所有备份，从而维护数据的一致性。对于多个相互备份的Chunk从中选取一个作为主备份，其余为次级备份，主备份决定次级备份的写入顺序

<img width="407" alt="18ef4996ef76aeb3e3987b11221568f" src="https://github.com/user-attachments/assets/f48dd6e1-9b33-4b9f-be21-e298785442a2">


## Colossus：下一代GFS
* Colossus对GFS的改进：
  * GFS主控服务器内存大小决定整个文件系统可以管理文件的数量，扩展性差。Colossus将单一主控服务器改造为多主控服务器构成的集群，将所有管理数据进行数据分片后分配到不同的主控服务器中
  * GFS通过Chunk数据备份来保证数据高可用性，但付出更多存储成本。 Colossus采用纠删码算法，可以在减少备份数目的情形下达到类似的高可用性要求
  * GFS在内部管理Chunk备份存放位置，客户端对此是不可见的，Colossus 增加了客户端的灵活性使得客户端可以管理备份数据的存储地点

## 纠删码(Erasure Code) 
* 为了增加存储系统的可靠性和数据可用性，经常使用数据备份来达到这一点，直接增加存储成本。能否在不对数据做备份的情形下提供类似的数据可用性？
  * 纠删码就是为了达到这一目的而在分布式存储中开始被广泛使用
* 纠删码通过对原始数据进行校验并保留校验数据，以增加冗余的方式来保证数据的可恢复性。
* 基本思想：将数据文件切割为等长的n个数据块，并根据n个数据块生成m个冗余的校验信息，得到n+m块数据。如果其中m块数据丢失，可以通过剩下的n个数据块将其进行恢复。

<img width="419" alt="2a84b39220a670ea26dc991208520bf" src="https://github.com/user-attachments/assets/11cbffc9-30db-40c7-83e3-22f8183dd65e">


## Reed-Solomon编码
* Reed-Solomon编码是最常用的纠删码之一，Google 在Colossus以及Facebook在HDFS-RAID中都已经实现并部署了RS编码来减少存储成本。如果要通过RS编码来计算原始数据的校验信息C，需要设定涉及所有原始数据块的编码函数F

<img width="761" alt="91332f7cceca12c21d429caabb0c39a" src="https://github.com/user-attachments/assets/807b0ed4-e879-49d7-848b-ba30e15bc309">

* RS编码的问题转换：已知n个数据字d1, d2, …, dn分别行储在不同的存储设备上，需要根据这些数据字计算m个校验字c1, c2, …, cm使得n+m个数据字可以容忍最多m个数据损失。
* 要实现这一点，RS编码涉及3 个主要问题：
  * 计算原始数据的校验字
  * 从数据错误中恢复原始数据。
  * 实现快速计算

## LRC编码
* 使用RS编码时，如果将一个文件划分成10个数据块，即使只有其中一个数据块损坏，也需要其他所有数据块来共同恢复这个损坏的数据块。在分布式网络环境下，为了恢复少量数据块，需要大量的网络传输和磁盘IO才能够将其进行恢复
* LRC的提出就是为了解决这一难题，所以LRC面临的问题是：能否在可靠性与RS编码大致相同的情况下，减少恢复损毁数据所需的数据块数量
* Facebook的XorBas系统采用了LRC 来达到这一目标，根据 XorBas的数据，通过采用LRC编码，其恢复损毁数据所需的数据块数量可以降低为RS编码的一半，付出的代价是增加14%的额外存储成本
* LRC编码中存在“块局部性（Block Locality）” 和“最小码距（Minimal Code Distance）”两个概念：
  * 块局部性：指的是对于某个纠删码来说，要对一个数据块编码，最多需要多少其他数据块。恢复某个数据块时需要其他数据块的个数
  * 最小码距：指的是对于切割成n块的文件，最少损毁多少块数据文件就不可恢复了

<img width="760" alt="3e1a5c95048b527c1ce1215761ae394" src="https://github.com/user-attachments/assets/e8a608ca-dd9e-4465-b507-767a5a59b490">

* LRC本质上是在RS编码基础上通过增加数据冗余来换取校验数据的局部性

## Pyramid编码
* 在组内有多个块失效时，LRC编码仍然需要在全局范围内进行修复。为此，可以对数据块进行更 多层次的分组，高层的组包含若干低层较小的组

<img width="747" alt="238d85a5f67e25421cbe409c94d85e8" src="https://github.com/user-attachments/assets/5c7452d2-cb14-4612-986f-f43cb75d881b">


## 层次分组编码
* LRC编码与Pyramid编码均属于层次分组编码
* 思想：将所有数据块分成少数几个较大且互不重叠的组，然后每个组再进一步分成几个若干互不重叠的子组。以此类推，可以根据需要划分多个层次。最后，每组各产生一个只与组内数据块相关的局部校验块，整体再产生若干与所有数据块相关的全局校验块

## 纠删码进行数据容错的挑战 
* 当前，纠删码容错技术在大规模分布式存储中面临的主要挑战已不再是其较高的运算复杂度，而是其较高的网络资源消耗，以及实现上的复杂性
* 纠删码容错技术面临的挑战主要表现在编码实现、数据修复和数据更新３个方面
  * 编码实现：主要挑战在于如何在编码实现完全完成之前保证数据的可靠性
  * 数据修复：挑战主要在于如何降低数据读取量和数据传输量。，从数据修复的具体过程入手，通过优化修复时的数据读取和数据传输过程
  * 数据更新：在基于纠删码容错的数据更新中，需要先将原数据读取出来，重新计算相应的校验数据，然后再写入。数据块关联的校验块较多，需要更新的编码块更多

