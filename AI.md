# 电计2203 符煜非 20221071292
# 问题
## 问题描述:
* 将写作过程的具体与写作质量联系起来，使用打字行为来预测论文质量，在本次比赛中的工作将使用击键日志数据中的过程特征来预测整体写作质量。这些努力可以确定学习者的写作行为和写作表现之间的关系。此外，鉴于目前的大多数写作评估工具主要集中在最终的书面产品上，这可能有助于将学习者的注意力引导到他们的文本制作过程中，并提高他们在写作中的自主性、元认知意识和自我调节能力评价。
* 指标使用均方根误差RMSE评估。
## 数据集描述：
* 数据集包含大约 5000 条用户输入日志，例如在论文撰写过程中采集的击键和鼠标点击。每篇文章的评分范围为 0 到 6。目标是预测文章从其用户输入日志中获得的分数
* 具体数据集描述如下图所示

<img width="660" alt="85a9238a41e2cb26894a2cdafad599b" src="https://github.com/user-attachments/assets/3e74a1da-480c-4127-bd14-9fd9fa6579d6">

<img width="805" alt="a64bc57765a1e0d64f6b427c53d64ca" src="https://github.com/user-attachments/assets/ec6c5e90-937f-4353-8139-e4f6aa81908f">

<img width="193" alt="98621e5280b0690b71c5c2a68a54274" src="https://github.com/user-attachments/assets/d9e53c17-6a4d-45ef-8b2a-1cde22c699be">


# 解决方案
## 数据预处理与特征工程
* 对写作者的活动进行统计，统计每个用户在不同行为上的活动次数('Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste')
* 对写作者的按键事件进行统计，比如输入某些字符，空格， 回退，上下左右箭头以及切换大小写等，这些按键可能反映了写作者的操作习惯和注意力集中程度。
* 对写作者的文本变更次数以及标点符号数量进行统计。
* 获取写作者的有效输入词，过滤掉无效的文本变更（如NoChange），连接文本并提取输入的有效单词，算单词数量、平均长度、最大长度和标准差等统计信息
* 同时添加一些各种比率特征来反映单位时间内的数据表示，如单词数量，事件数量与时间的比率。
* 捕获写作者的时间滞后特征，分析用户在不同时间间隔内的行为变化，能够展示出用户在不同时间点之间的输入延迟，能够反映他们的思考和反应时间
* 捕获写作者的光标位置滞后特征，分析用户光标位置的变化情况，用户在输入时如何移动光标，可能揭示其输入习惯或效率。

## 模型的选择
* 常见的神经网络模型对于这种问题的表格型数据在我的试验中表现得并不优秀，放弃使用NN相关的算法，转而选择GBDT系列的相关算法。
* 根据Disscussion提供的数据(如下图)和我本地使用optuna对不同模型对比不断超参数调优，LightGBMRegressor确实表现优异。

<img width="175" alt="104064504f46313c541fdb4cd20d644" src="https://github.com/user-attachments/assets/f3db1ed5-8d34-471b-88d7-e5c2ef597eca">

## 对模型的理解
* LightGBM可以算是决策树的超集了，在应对此类数据的机器学习算法中比较常用。
* 作为GBDT框架的算法的一员，并且作为XGB算法的后来者，LGBM非常好综合了包括XGB在内的此前GBDT算法框架内各算法的一系列优势，并在此基础上做了一系列更进一步的优化
* GBDT使用梯度提升框架来构建决策树，它通过不断添加新的模型来修正之前模型的错误，每个新的决策树都用来拟合之前所有树的残差，LightGBM是一个基于GBDT的改进算法，它采用了基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）两种技术来优化计算，这些优化使得LightGBM在处理大规模数据集时更加高效。
* 相比于GBDT，LightGBM使用了直方图算法,它的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。
* 在 Histogram 算法之上，LightGBM 进行进一步的优化。首先它抛弃了大多数 GBDT 工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。Level-wise 过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上 Level-wise 是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，Leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。Leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。

## 模型的训练与评估
* 用optuna得到的最好超参数加到LightGBM模型中，将原有的数据分割为训练集与测试集，设置比例为3：7。
* 将训练集用于全新的模型训练，将测试集测试得到的最优结果Root Mean Squared Error (RMSE): 0.6417039593649891
* 通过查看模型的特征重要性可以对模型进行进一步的改善和优化。

<img width="886" alt="94b6f67dc418a0258ad85d7eafe4e8e" src="https://github.com/user-attachments/assets/24ec93f8-b09a-4c11-9b04-547a1459e6bd">

<img width="874" alt="f160e367116e3f78b4af5c03c4789e1" src="https://github.com/user-attachments/assets/cd9adbe2-dd43-4f14-99da-279d30cd8c14">

## 总结
* 模型在预测低分数论文与高分数论文的表现并不相同。低分可能不是由于糟糕的 “类型技巧” 造成的。由于论文是根据其内容进行评分的，因此偏离主题的答案或不易发现的语法错误可能会被扣分。是否应该过度关注文本可能蕴藏的内容有所探究。
* 甚至在把匿名文本q字符转换为aa会显著提高模型的表现令我费解。
* 对于论文评分的主观性，我认为RMSE可能不是一个最佳的指标选择。
