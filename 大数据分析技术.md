# 1 -- 从数据分析到大数据
## 什么是数据分析，有什么用？
* 数据分析：用适当的统计分析方法对收集来的大量数据进行分析，提取有用信息和形成结论而对数据加以详细研究和概括总结的过程

## 计算机时代的数据分析
* 随着数据库系统的广泛应用，数据库在提供丰富信息的同时，也体现出海量信息的特征。随着信息爆炸，有效信息难以提炼的问题日益显著，由此陷入“数据丰富而知识贫乏”的窘境。

## 数据挖掘
* 数据挖掘：从大量的数据中通过算法搜索隐藏于其中信息的过程，是数据库知识发现中的一个步骤。通常与计算机科学有关，并通过统计、在线分析处理、情报检索、机器学习、专家系统（依靠过去的经验法则）和模式识别等诸多方法来实现上述目标

<img width="772" alt="70f89718cc48560f1097ae8b2e9fd9e" src="https://github.com/user-attachments/assets/cae2b937-3f3c-4a8d-97c9-cfb063654194">


## 数据仓库
* 数据仓库：为企业的决策制定，提供所有类型数据支持的战略集合。它是单个数据存储，出于数据分析和决策支持目的而创建。 为需要业务智能的企业，提供业务流程改进、成本质量控制等方面的指导。

<img width="702" alt="c063b07db8006710faaf77893fa0772" src="https://github.com/user-attachments/assets/80653cab-e51c-4156-b595-47b87d46d6db">


## 互联网对数据分析的挑战

<img width="771" alt="9a3e03dbc4f2cffe7d6ab2174122b27" src="https://github.com/user-attachments/assets/6dc4db28-52d0-43f1-a048-7c435c84d35e">


## 大数据的定义
<img width="778" alt="d0e5d214c0abd74c0700cbda3aaff56" src="https://github.com/user-attachments/assets/cd8f0279-7965-4fe8-ba83-28bb440b44fd">


## 大数据 V.S. 常规数据 
<img width="764" alt="0cd71e48075c1547f16beec9b4ffb9d" src="https://github.com/user-attachments/assets/4b2c1283-4c59-40a8-9e6c-d3e31b76c482">

# 2-大数据应用
## 大数据的应用
* 感知现在：潜在线索与模式的挖掘，事件、群体与社会发展状态的感知。
  * 挑战：传统数据处理方法感知度量难、特征融合难、模式挖掘难
* 预测未来：揭示事物发展的演变规律，进而对事物发展趋势进行预测。
  * 挑战：传统数据处理方法的时效性与准确性难以兼顾、演变趋势难以预测

## 大数据到数据科学
* 数据科学：使用科学的方法从结构化数据和非结构化数据中，提取精准的知识和见解。同时，在产生这些数据的应用领域中，基于知识和见解来制定更好、更为有效的决策方案。

<img width="758" alt="131c95b8cfad9d405e0c020b1b5f80c" src="https://github.com/user-attachments/assets/66995cb7-65aa-4158-bb1a-88a92a78d1c3">

## 大数据分析推动“智能+”
* 人工智能=训练大数据+高性能并行计算+规模化灵巧的算法
* 深度学习=许多训练数据+并行计算+规模化灵巧的的算法 

# 3-数据采集1
## 数据的重要性
* 数据是构建 AI 系统所必需的关键基础设施。因为数据在很大程度上决定了 AI 系统的性能、公平性、稳健性、安全性和可扩展性

## 数据采集的挑战 
* 传统数据源对数据采集支持较差：
  * 业务数据库是为了支撑业务，数据表关联复杂，且一般针对高并发及低延迟的小操作进行设计
  * Web日志往往是为了方便业务调试来做。
* 数据团队与业务团队配合困难：
  * 数据分析让路产品升级；
  * 对于业务团队而言，数据采集属于额外工作

## 数据采集遵循法则 
* 海量数据，包括业务的各个角度
* 多种数据源，全量数据，全面覆盖
* 按需求将不同维度的数据都进行采集。
* 强调时效性，实时采集保障分析价值

## 埋点(用户行为数据采集)
* 埋点：在正常的业务逻辑中，嵌入数据采集的代码
* 埋点优势：数据是手动编码产生的，易于收集，灵活性大，扩展性强
* 埋点劣势：必须十分清楚目标，即需要收集什么样的数据必须提前确定；容易发生漏埋现象；产品迭代过程中，忽略了埋点逻辑的更改
* 埋点方式有哪些？
  * 全埋点/无埋点：“全部采集，按需选取”；在产品中嵌入SDK，做个统一埋点，一般用于采集APP的用户行为
  * 可视化埋点：在全埋点部署成功、可以获得全量数据的基础上，以可视化的方式，在对应页面上定义想要的页面数据，或者控件数据
  * 代码埋点：前端代码埋点和后端代码埋点。更适合精细化分析的场景，采集各种细粒度数据
  * 前两种方式是无埋码实现数据采集，比较适合给运营和市场人员使用；第三种需要技术人员通过编码来实施

* 无埋点优势：技术门槛低，部署使用简单；用户友好性强；收集的是全量数据，不会出现漏埋、误埋等现象
* 无埋点劣势：适用通用场景、标准化采集，自定义属性采集覆盖不了；采集全量数据，给数据传输和服务器增加压力；兼容性有限等
* 埋点采集数据的过程

<img width="730" alt="73709b3fd4fc62440b44eca8f25566f" src="https://github.com/user-attachments/assets/59ef9cb6-e98e-4cb7-a5e4-bebd754a94ed">

* 埋点方案应具备四个要素：
  * 确认事件与变量：事件指产品中的操作，变量指描述事件的属性。按照产品流程来设计关键事件
  * 明确事件的触发时机：不同触发时机代表不同的数据统计口径，要尽量选择最贴近业务的统计口径，然后再与开发沟通
  * 规范命名：对事件进行规范统一的命名，有助于提高数据的实用性及数据管理效率
  * 明确优先级：在设计埋点方案时，一定要对埋点事件有明确的优先级排布

## ETL(Extract-Transform-Load) (系统业务数据整合)
* ETL：用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程
* ETL是将业务系统的数据经过抽取、清洗转换之后加载到数据仓库的过程，目的是将企业中的分散、零乱、标准不统一的数据整合到一起，为企业的决策提供分析依据。是BI的重要一环，在BI项目中占至少三分之一的时间

<img width="776" alt="138c74b21bc9fcb77c2e6590886896e" src="https://github.com/user-attachments/assets/b17e83e3-13e9-4317-bf9b-c7ea0e4643d5">

* ETL的设计分为三部分：数据的抽取、数据的清洗转换、数据的加载。花费时间最长的是Transform（清洗转换）部分，一般情况下是整个ETL的2/3
* ETL常用的三种实现方法：借助ETL工具；SQL方式实现；ETL工具和SQL相结合
* ETL工具解决的问题：数据来自不同物理主机、数据来自不同数据库或者文件、异构数据处理等

### ETL——数据抽取
<img width="753" alt="1a60cf667388c267dace267960c323a" src="https://github.com/user-attachments/assets/c080564e-a57d-47fd-89e3-91433be9b724">

### ETL——数据转换
<img width="773" alt="19be13247428d5047191ad493df4bf8" src="https://github.com/user-attachments/assets/090c02f9-5904-49de-ab97-8d67addcd254">


### ETL——数据加载
<img width="757" alt="6bac7688a3f5d9b3fb7d3809afa1372" src="https://github.com/user-attachments/assets/3b21cf80-1e6b-451b-bc62-46a7f7938d06">

### 常用的ETL工具
* Kettle
* Apatar
* Scriptella
* ETL Automation

## 网络爬虫(互联网数据采集)
* 网络爬虫：是一种按照一定的规则，自动抓取万维网信息（网页）的程序或者脚本。为搜索引擎从万维网上抓取网页，是搜索引擎的重要组成部分
* 网络爬虫可分为通用网络爬虫和聚焦网络爬虫

<img width="758" alt="80bef729b558979181d113d7bf3574c" src="https://github.com/user-attachments/assets/3de3e55a-d4e5-49d2-8f95-2a4cb2233906">

<img width="762" alt="35fca274f6f609a7b8ba4e974b4f7d2" src="https://github.com/user-attachments/assets/ed34165a-3556-468e-aec3-acacbaf8de67">

* 网络爬虫视角下的网页：

<img width="707" alt="d163df8cc73b28de539db47a6d84fde" src="https://github.com/user-attachments/assets/a3e42158-b7af-4149-94cb-64924154ba02">

* 网络爬虫的抓取策略：
  * 深度优先遍历策略：从起始页开始，一个个链接跟踪下去。
  * 宽度优先遍历策略：抓取当前网页中链接的所有网页，再从待抓取队列中选取下一个URL
  * 反向链接数策略：反向链接数是指一个网页被其他网页链接指向的数量。使用这个指标评价网页的重要程度，从而决定抓取先后顺序
  * 基于优先级计算的策略：针对待抓取网页计算优先级值，通过排序来确定抓取顺序。如：Partial PageRank，OPIC等
  * 大站优先策略：对于待抓取队列中的所有网页，根据所属的网站进行分类，对于待下载页面数多的网站优先下载

* 定期更新策略
  * 历史参考策略：在网页的历史更新数据基础上，利用建模等手段，预测网页下一次更新的时间，确定爬取周期
  * 用户体验策略：依据网页多个历史版本的内容更新、搜索质量影响、用户体验等信息，来确定爬取周期
  * 聚类分析策略：首先对海量的网页进行聚类分析，每个类中的网页一般具有类似的更新频率。通过抽样计算，确定针对每个聚类的爬行频率

* 网络爬虫系统架构：往往是一个分布式系统。

<img width="773" alt="cc8249caeddd5e1afea9e8caee013ca" src="https://github.com/user-attachments/assets/60030490-8d48-4866-9141-11e0ed2f6696">

<img width="777" alt="aca30d90fd6b721bc9090518b38982f" src="https://github.com/user-attachments/assets/fce6adb4-db88-4426-b56d-d1dec07187d7">

<img width="802" alt="136c6c96b622180a1ac004082d8d080" src="https://github.com/user-attachments/assets/60f9af3b-0a6d-4176-ba4b-cded91cd1d65">


# 4-数据采集2
## Apache Flume(日志数据采集)
* Flume是分布式、可靠、和高可用的海量日志采集、聚合和传输的日志收集系统。由Cloudera公司开源，连接数据源和数据存储系统的管道，屏蔽数据源和数据存储系统异构性的中间件
* Flume-OG基本架构

<img width="715" alt="b8a2c5565c3f1bbf53f99c8ea11e045" src="https://github.com/user-attachments/assets/67adb078-fe63-4662-a501-c698cc271a5a">

* 基于Flume-OG的美团日志收集系统

<img width="757" alt="4f0a216f5bfe8db62b431fa7554793d" src="https://github.com/user-attachments/assets/ebc0624e-d3ba-4c23-bcd0-486dea13acde">


* Flume-NG基本架构

<img width="774" alt="e0a6b5a0c619a15b8f098ff515b4695" src="https://github.com/user-attachments/assets/a50a5431-6f57-4173-8017-4a4a93722028">

* Flume-NG核心概念
  * Event：Flume数据传输的基本单元，由可选的header和载有数据的byte array构成， byte array可以携带日志数据。
  * Client：将原始日志文件包装成Events并发送它们到一个或多个Agent实体，由独立的线程运行
  * Agent：Flume的运行实体，包含Source, Channel, Sink等组件。利用这些组件将Events从一个节点传输到另一个节点或最终目的地。每台机器运行一个Agent
    * Source：负责接收Event或通过特殊机制产生Event，并将Events批量的放到一个或多个Channel
    * Channel：连接Source和Sink，类似event的缓存队列
    * Sink：接收Event，进行下一步转发

* Flume-OG V.S Flume-NG
  * Flume-OG有三种角色的节点， Flume-NG只有一种角色节点，降低臃肿性，更加灵活
  * 在OG版本中，Flume 的使用稳定性依赖 zookeeper；而在NG版本中，不再需要zookeeper对各类节点进行协调
  * 在Flume-NG中，agent节点的组成也发生了变化，由 source、sink、channel组成

* Flume拓扑结构

<img width="769" alt="a135a6a3718de50c9fc07bba20a2801" src="https://github.com/user-attachments/assets/df5b91ef-9394-477b-93e9-39f10d42dce8">

<img width="782" alt="ca5b9efba1d373ceaa82a77a3215af1" src="https://github.com/user-attachments/assets/c9874cc9-95d4-40dc-a032-428750bf57f9">

<img width="785" alt="3fddca163984cf73933332923560d3e" src="https://github.com/user-attachments/assets/2cf7e3d3-558c-4d41-9699-da0e236cd2d4">

<img width="776" alt="9fe87524407ec188a465bcaf0fa3c0e" src="https://github.com/user-attachments/assets/c4a2b7bd-b700-4514-b310-46a6e992a388">


## Apache Kafka(数据分发中间件)
* 为什么需要数据分发？
  * 前端数据采集后，需要送到后端进行分析处理。前端采集与后端处理往往是多对多的关系。之间需要数据分发中间件负责消息转发，保障消息可靠性，匹配前后端速度差
  * 分布式系统构件之间通过数据分发可以解除相互之间的功能耦合，从而减轻子系统之间的依赖，使得各个子系统或构件可以独立演进、维护或者重用

* 数据分发解决方案——消息队列
  * 消息队列是在消息传输过程中保存消息的容器或中间件，主要目的是提供消息路由并保障消息可靠传递
  * 目前常见的消息队列中间件产品包括：ActiveMQ 、ZeroMQ,RabbitMQ和Kafka
  * 一般消息中间件支持两种模式：消息队列模式及PubSub模式。

* Kafka：分布式发布-订阅消息系统（Pub-Sub模式），最初由LinkedIn公司开发，之后成为Apache项目的一部分。具有极高的消息吞吐量，较强的可扩展性和高可用性，消息传递低延迟，能够对消息队列进行持久化保存，且支持消息传递的"至少送达一次" 语义

* Kafka架构

<img width="763" alt="3501bd7e947da5e8e35c18cff929959" src="https://github.com/user-attachments/assets/cf5d63bc-408d-4277-8412-732ca7d1d9c7">

* 基本概念——Topics & Partition
  * Topics是消息的分类名（或Feed的名称），一个Topic可以认为是一类消息，每个topic将被分成多个partition(区)。partition是以log文件的形式存储在文件系统中，任何发布到partition的消息都会被直接追加到log文件的尾部。Logs文件根据配置要求,保留一定时间后删除来释放磁盘空间。
  * Partition： Topic物理上的分组，一 个 topic可以分为多个partition，每个 partition是一个有序的队列。partition中的每条消息都会被分配一个有序的 id（offset）。
  * Partition设计目的：
    * 通过分区,可以将日志内容分散到多个server上,来避免文件尺寸达到单机磁盘的上限,每个partiton都会被当前server(kafka实例)保存
    * 可以将一个topic切分多任意多个partitions,来提升消息保存/消费的效率
    * 越多的partitions意味着可以容纳更多的consumer，有效提升并发消费的能力

  
<img width="449" alt="9f8f7dae245ec35f989063b274a5a8e" src="https://github.com/user-attachments/assets/511b9e26-3ac7-48a2-afa8-56942905edaa">


* 基本概念——Producer
  * Producer 将消息发布到指定的Topic中,同时Producer 也能决定将此消息归属于哪个partition;比如基于"round-robin"方式或者通过其他的一些算法等
  * 消息和数据生产者，向 Kafka的一个topic发布消息的过程叫做 producers
  * 异步发送：批量发送可以很有效的提高发送效率。Kafka producer的异步发送模式允许进行批量发送，先将消息缓存在内存中，然后一次请求批量发送出去。

* 基本概念——Consumer
  * 消息和数据的消费者，订阅相关topics，并处理Producer发布的消息
  * 允许consumer group（包含多个consumer）对一个topic进行消费，不同的consumer group之间独立订阅
  * 每个consumer属于一个consumer group；发布的消息,只会被订阅此Topic的每个group中的一个consumer消费
  * 同一个group中不能有多于partitions个数的consumer同时消费,否则将意味着某些consumer将无法得到消息。
 
* 基本概念——Broker
  * Broker：缓存代理，Kafka 集群中的一台或多台服务器统称为 broker
  * 为了减少磁盘写入的次数，broker会将消息暂时buffer起来，当消息的个数(或尺寸)达到一定阀值时，再flush到磁盘，这样减少了磁盘IO调用的次数。
  * Broker不保存订阅者的状态，由订阅者自己保存。Broker没有副本机制，一旦broker宕机，该broker的消息将都不可用

* 基本概念——Message
  * Message消息：是通信的基本单位，每个 producer 可以向一个 topic（主题）发布一些消息
  * Kafka中的Message是以topic为基本单位组织的，不同的topic之间是相互独立的。每个topic又可以分成几个不同的partition(每个topic有几个partition是在创建topic时指定的)，每个partition存储一部分Message
  * partition中的每条Message包含了以下三个属性：
    * offset 对应类型：long
    * MessageSize 对应类型：int32
    * data 是message的具体内容

* Kafka的消息发送的流程

<img width="740" alt="280db882d7df1fd5e8ead7a9bf8808f" src="https://github.com/user-attachments/assets/334519ac-f52c-4e15-9e0b-ef52fcd486b2">

* Kafka的消息流示例

<img width="758" alt="9560364fa3ff29d126d7fb3673e6f3b" src="https://github.com/user-attachments/assets/1687b62a-26d9-4a96-990b-ce8a41e00e96">

* Kafka通过消息副本机制提供了高可用的消息服务， 其副本管理单位不是Topic消息队列，而是Topic的数据分片(Partition) 。在配置文件里可以指定数据分片的副本个数
* 在多个副本里，其中一个作为主副本(Leader)，其他作为次级副本(Slave) 。所有针对这个数据分片的消息读/写请求都由主副本来负责响应，次级副本只是以主副本数据消费者的方式从主副本同步数据

* Kafka是一个基于文件系统的消息系统，那么基于磁盘读写操作的消息系统如何保证系统性能？由于磁盘读/写因为涉及寻道操作这种机械运动，所以和内存读/写相比其效率极低。为此，考虑如何运用磁盘本身的特性来极大提升系统效率？
  * 原则：尽可能避免随机读/写，同时尽可能利用顺序读/写， 即连续读/写整块数据,Kafka能够高效处理大批量消息的一个重要原因就是将读/写操作尽可能转换为顺序读/写

## 其他数据采集
* 探针：网络流量数据捕获
* 传感器：环境数据捕获
* RFID Reader：标签数据捕获

# 5-分布式存储
## 大数据存储
* 重要手段：超大规模分布式文件系统，即以文件系统的方式来组织海量数据
* 代表工作：Google的文件系统——GFS

## Google文件系统(GFS )
* GFS 提供了海量非结构化信息的存储平台，并提供数据的冗余备份、成千台服务器的自动负载均衡以及失效服务器检测等各种完备的分布式存储功能
* 在GFS基础功能之上，可以开发更加符合应用需求的存储系统和计算框架

## 文件系统
* 文件系统是操作系统用于明确存储设备或分区上的文件的方法和数据结构，即在存储设备上组织文件的方法

## GFS设计原则 
* GFS采用大量商业PC来构建存储集群，PC的稳定性并没有很高的保障。部件错误不再被当作异常，而是将其作为常见的情况加以处理。数据冗余备份、机器有效性检测、机器故障恢复都被放在设计目标里
* GFS文件系统所存储的文件绝大多数都是大文件（GB），设计时针对大文件的读写操作进行了优化，尽管也支持小文件，但不作为重点
* 系统中存在大量的“追加写” 操作。即将新增内容追加到已有文件末尾，已经写入的内容一般不做更改，很少有文件的"随机写"行为
* 对于数据读取操作来说，绝大多数读文件操作都是“顺序读”，少量操作是“随机读”。往往是一次读取较大量的数据，而不是不断定位到某个位置读取少量数据

## GFS整体架构
* GFS文件系统主要由3个组成部分构成：唯一的主控服务器(Master)、众多的Chunk服务器和GFS客户端
  * 主控服务器：负责整体系统的管理工作
  * Chunk服务器：负责实际的数据存储并响应客户端的读/写请求
  * GFS客户端：用户操作

* GFS由上千台PC构成，但是在应用开发者的眼中，GFS类似于本地的统一文件系统，分布式存储系统的细节是隐藏的
* GFS文件存储：对于开发者而言，GFS类似于Linux文件系统或者是 Windows操作系统提供的文件系统，由目录和存放在目录下的文件构成树形结构，这个树形结构被称为GFS命名空间。GFS向应用开发者提供了文件的创建、删除、读取和写入等常见的操作接口(API)

<img width="422" alt="786ad57f1916cfb78dfe3f54ea13fc7" src="https://github.com/user-attachments/assets/169d9132-3e10-413c-9432-d338c7a74724">

* GFS命名空间由众多的目录和GFS文件构成， 一个GFS文件由众多固定大小的Chunk构成。而每个Chunk又由更小的粒度Block 构成，Chunk是基本存储单元，Block是基本读取单元

<img width="414" alt="ef18ad7a23e6775f9e61affb082d830" src="https://github.com/user-attachments/assets/534a543f-ab5c-45ce-bb0a-3160abac8d73">

## GFS主控服务器 
* GFS主控服务器所管理的系统元数据包括以下三种：
  * GFS命名空间和Chunk命名空间：主要用来对目录文件以及Chunk的增删改等信息进行记录
  * 从文件到其所属Chunk之间的映射关系：因为一个文件会被切割成众多Chunk，所以系统需要维护这种映射关系
  * Chunk在Chunk服务器上的存储信息：每个Chunk会被复制多个备份，并存储在不同的服务器上

<img width="428" alt="7c0f0d7b9cbc8cfa4d05069cce768d8" src="https://github.com/user-attachments/assets/293abf09-9676-4f5c-91c0-31cbe4714d2b">

* 主控服务器上管理数据的安全性需要保障：
  * GFS将前两类管理信息(命名空间，文件到Chunk的映射) 记录在系统日志文件内。并且将系统日志分别存储在多台机器上， 避免信息丢失。
  * 第3类管理信息(Chunk存储信息)通过主控服务器定期询问Chunk服务器来维护的

* 主控服务器所承担的系统管理工作：
  * 不同Chunk服务器之间的负载均衡。创建新的Chunk以及垃圾回收
  * 数据备份及迁移时需要考虑
    * Chunk数据的可用性；当Chunk数据不可用时，及时重新备份
    * 尽可能减少网络传输压力

* GFS是典型的主从结构方式。通过主控服务器实现全局管控，管理便捷
  * 缺点：系统瓶颈，单点失效
* GFS采用增加另外一台影子服务器(Shadow)的方式来解决单点失效问题。当主控服务器出现故障，无法提供服务时。由影子服务器接替主控服务器行使对应的管理功能

## GFS写操作示例
* GFS客户端发出写操作请求，GFS系统必须将这个写操作应用到 Chunk的所有备份，从而维护数据的一致性。对于多个相互备份的Chunk从中选取一个作为主备份，其余为次级备份，主备份决定次级备份的写入顺序

<img width="407" alt="18ef4996ef76aeb3e3987b11221568f" src="https://github.com/user-attachments/assets/f48dd6e1-9b33-4b9f-be21-e298785442a2">


## Colossus：下一代GFS
* Colossus对GFS的改进：
  * GFS主控服务器内存大小决定整个文件系统可以管理文件的数量，扩展性差。Colossus将单一主控服务器改造为多主控服务器构成的集群，将所有管理数据进行数据分片后分配到不同的主控服务器中
  * GFS通过Chunk数据备份来保证数据高可用性，但付出更多存储成本。 Colossus采用纠删码算法，可以在减少备份数目的情形下达到类似的高可用性要求
  * GFS在内部管理Chunk备份存放位置，客户端对此是不可见的，Colossus 增加了客户端的灵活性使得客户端可以管理备份数据的存储地点

## 纠删码(Erasure Code) 
* 为了增加存储系统的可靠性和数据可用性，经常使用数据备份来达到这一点，直接增加存储成本。能否在不对数据做备份的情形下提供类似的数据可用性？
  * 纠删码就是为了达到这一目的而在分布式存储中开始被广泛使用
* 纠删码通过对原始数据进行校验并保留校验数据，以增加冗余的方式来保证数据的可恢复性。
* 基本思想：将数据文件切割为等长的n个数据块，并根据n个数据块生成m个冗余的校验信息，得到n+m块数据。如果其中m块数据丢失，可以通过剩下的n个数据块将其进行恢复。

<img width="419" alt="2a84b39220a670ea26dc991208520bf" src="https://github.com/user-attachments/assets/11cbffc9-30db-40c7-83e3-22f8183dd65e">


## Reed-Solomon编码
* Reed-Solomon编码是最常用的纠删码之一，Google 在Colossus以及Facebook在HDFS-RAID中都已经实现并部署了RS编码来减少存储成本。如果要通过RS编码来计算原始数据的校验信息C，需要设定涉及所有原始数据块的编码函数F

<img width="761" alt="91332f7cceca12c21d429caabb0c39a" src="https://github.com/user-attachments/assets/807b0ed4-e879-49d7-848b-ba30e15bc309">

* RS编码的问题转换：已知n个数据字d1, d2, …, dn分别行储在不同的存储设备上，需要根据这些数据字计算m个校验字c1, c2, …, cm使得n+m个数据字可以容忍最多m个数据损失。
* 要实现这一点，RS编码涉及3 个主要问题：
  * 计算原始数据的校验字
  * 从数据错误中恢复原始数据。
  * 实现快速计算

## LRC编码
* 使用RS编码时，如果将一个文件划分成10个数据块，即使只有其中一个数据块损坏，也需要其他所有数据块来共同恢复这个损坏的数据块。在分布式网络环境下，为了恢复少量数据块，需要大量的网络传输和磁盘IO才能够将其进行恢复
* LRC的提出就是为了解决这一难题，所以LRC面临的问题是：能否在可靠性与RS编码大致相同的情况下，减少恢复损毁数据所需的数据块数量
* Facebook的XorBas系统采用了LRC 来达到这一目标，根据 XorBas的数据，通过采用LRC编码，其恢复损毁数据所需的数据块数量可以降低为RS编码的一半，付出的代价是增加14%的额外存储成本
* LRC编码中存在“块局部性（Block Locality）” 和“最小码距（Minimal Code Distance）”两个概念：
  * 块局部性：指的是对于某个纠删码来说，要对一个数据块编码，最多需要多少其他数据块。恢复某个数据块时需要其他数据块的个数
  * 最小码距：指的是对于切割成n块的文件，最少损毁多少块数据文件就不可恢复了

<img width="760" alt="3e1a5c95048b527c1ce1215761ae394" src="https://github.com/user-attachments/assets/e8a608ca-dd9e-4465-b507-767a5a59b490">

* LRC本质上是在RS编码基础上通过增加数据冗余来换取校验数据的局部性

## Pyramid编码
* 在组内有多个块失效时，LRC编码仍然需要在全局范围内进行修复。为此，可以对数据块进行更 多层次的分组，高层的组包含若干低层较小的组

<img width="747" alt="238d85a5f67e25421cbe409c94d85e8" src="https://github.com/user-attachments/assets/5c7452d2-cb14-4612-986f-f43cb75d881b">


## 层次分组编码
* LRC编码与Pyramid编码均属于层次分组编码
* 思想：将所有数据块分成少数几个较大且互不重叠的组，然后每个组再进一步分成几个若干互不重叠的子组。以此类推，可以根据需要划分多个层次。最后，每组各产生一个只与组内数据块相关的局部校验块，整体再产生若干与所有数据块相关的全局校验块

## 纠删码进行数据容错的挑战 
* 当前，纠删码容错技术在大规模分布式存储中面临的主要挑战已不再是其较高的运算复杂度，而是其较高的网络资源消耗，以及实现上的复杂性
* 纠删码容错技术面临的挑战主要表现在编码实现、数据修复和数据更新３个方面
  * 编码实现：主要挑战在于如何在编码实现完全完成之前保证数据的可靠性
  * 数据修复：挑战主要在于如何降低数据读取量和数据传输量。，从数据修复的具体过程入手，通过优化修复时的数据读取和数据传输过程
  * 数据更新：在基于纠删码容错的数据更新中，需要先将原数据读取出来，重新计算相应的校验数据，然后再写入。数据块关联的校验块较多，需要更新的编码块更多

