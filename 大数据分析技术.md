

# 1 -- 从数据分析到大数据
## 什么是数据分析，有什么用？
* 数据分析：用适当的统计分析方法对收集来的大量数据进行分析，提取有用信息和形成结论而对数据加以详细研究和概括总结的过程

## 计算机时代的数据分析
* 随着数据库系统的广泛应用，数据库在提供丰富信息的同时，也体现出海量信息的特征。随着信息爆炸，有效信息难以提炼的问题日益显著，由此陷入“数据丰富而知识贫乏”的窘境。

## 数据挖掘
* 数据挖掘：从大量的数据中通过算法搜索隐藏于其中信息的过程，是数据库知识发现中的一个步骤。通常与计算机科学有关，并通过统计、在线分析处理、情报检索、机器学习、专家系统（依靠过去的经验法则）和模式识别等诸多方法来实现上述目标

<img width="772" alt="70f89718cc48560f1097ae8b2e9fd9e" src="https://github.com/user-attachments/assets/cae2b937-3f3c-4a8d-97c9-cfb063654194">


## 数据仓库
* 数据仓库：为企业的决策制定，提供所有类型数据支持的战略集合。它是单个数据存储，出于数据分析和决策支持目的而创建。 为需要业务智能的企业，提供业务流程改进、成本质量控制等方面的指导。

<img width="702" alt="c063b07db8006710faaf77893fa0772" src="https://github.com/user-attachments/assets/80653cab-e51c-4156-b595-47b87d46d6db">


## 互联网对数据分析的挑战

<img width="771" alt="9a3e03dbc4f2cffe7d6ab2174122b27" src="https://github.com/user-attachments/assets/6dc4db28-52d0-43f1-a048-7c435c84d35e">


## 大数据的定义
<img width="778" alt="d0e5d214c0abd74c0700cbda3aaff56" src="https://github.com/user-attachments/assets/cd8f0279-7965-4fe8-ba83-28bb440b44fd">


## 大数据 V.S. 常规数据 
<img width="764" alt="0cd71e48075c1547f16beec9b4ffb9d" src="https://github.com/user-attachments/assets/4b2c1283-4c59-40a8-9e6c-d3e31b76c482">

# 2-大数据应用
## 大数据的应用
* 感知现在：潜在线索与模式的挖掘，事件、群体与社会发展状态的感知。
  * 挑战：传统数据处理方法感知度量难、特征融合难、模式挖掘难
* 预测未来：揭示事物发展的演变规律，进而对事物发展趋势进行预测。
  * 挑战：传统数据处理方法的时效性与准确性难以兼顾、演变趋势难以预测

## 大数据到数据科学
* 数据科学：使用科学的方法从结构化数据和非结构化数据中，提取精准的知识和见解。同时，在产生这些数据的应用领域中，基于知识和见解来制定更好、更为有效的决策方案。

<img width="758" alt="131c95b8cfad9d405e0c020b1b5f80c" src="https://github.com/user-attachments/assets/66995cb7-65aa-4158-bb1a-88a92a78d1c3">

## 大数据分析推动“智能+”
* 人工智能=训练大数据+高性能并行计算+规模化灵巧的算法
* 深度学习=许多训练数据+并行计算+规模化灵巧的的算法 

# 3-数据采集1
## 数据的重要性
* 数据是构建 AI 系统所必需的关键基础设施。因为数据在很大程度上决定了 AI 系统的性能、公平性、稳健性、安全性和可扩展性

## 数据采集的挑战 
* 传统数据源对数据采集支持较差：
  * 业务数据库是为了支撑业务，数据表关联复杂，且一般针对高并发及低延迟的小操作进行设计
  * Web日志往往是为了方便业务调试来做。
* 数据团队与业务团队配合困难：
  * 数据分析让路产品升级；
  * 对于业务团队而言，数据采集属于额外工作

## 数据采集遵循法则 
* 海量数据，包括业务的各个角度
* 多种数据源，全量数据，全面覆盖
* 按需求将不同维度的数据都进行采集。
* 强调时效性，实时采集保障分析价值

## 埋点(用户行为数据采集)
* 埋点：在正常的业务逻辑中，嵌入数据采集的代码
* 埋点优势：数据是手动编码产生的，易于收集，灵活性大，扩展性强
* 埋点劣势：必须十分清楚目标，即需要收集什么样的数据必须提前确定；容易发生漏埋现象；产品迭代过程中，忽略了埋点逻辑的更改
* 埋点方式有哪些？
  * 全埋点/无埋点：“全部采集，按需选取”；在产品中嵌入SDK，做个统一埋点，一般用于采集APP的用户行为
  * 可视化埋点：在全埋点部署成功、可以获得全量数据的基础上，以可视化的方式，在对应页面上定义想要的页面数据，或者控件数据
  * 代码埋点：前端代码埋点和后端代码埋点。更适合精细化分析的场景，采集各种细粒度数据
  * 前两种方式是无埋码实现数据采集，比较适合给运营和市场人员使用；第三种需要技术人员通过编码来实施

* 无埋点优势：技术门槛低，部署使用简单；用户友好性强；收集的是全量数据，不会出现漏埋、误埋等现象
* 无埋点劣势：适用通用场景、标准化采集，自定义属性采集覆盖不了；采集全量数据，给数据传输和服务器增加压力；兼容性有限等
* 埋点采集数据的过程

<img width="730" alt="73709b3fd4fc62440b44eca8f25566f" src="https://github.com/user-attachments/assets/59ef9cb6-e98e-4cb7-a5e4-bebd754a94ed">

* 埋点方案应具备四个要素：
  * 确认事件与变量：事件指产品中的操作，变量指描述事件的属性。按照产品流程来设计关键事件
  * 明确事件的触发时机：不同触发时机代表不同的数据统计口径，要尽量选择最贴近业务的统计口径，然后再与开发沟通
  * 规范命名：对事件进行规范统一的命名，有助于提高数据的实用性及数据管理效率
  * 明确优先级：在设计埋点方案时，一定要对埋点事件有明确的优先级排布

## ETL(Extract-Transform-Load) (系统业务数据整合)
* ETL：用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程
* ETL是将业务系统的数据经过抽取、清洗转换之后加载到数据仓库的过程，目的是将企业中的分散、零乱、标准不统一的数据整合到一起，为企业的决策提供分析依据。是BI的重要一环，在BI项目中占至少三分之一的时间

<img width="776" alt="138c74b21bc9fcb77c2e6590886896e" src="https://github.com/user-attachments/assets/b17e83e3-13e9-4317-bf9b-c7ea0e4643d5">

* ETL的设计分为三部分：数据的抽取、数据的清洗转换、数据的加载。花费时间最长的是Transform（清洗转换）部分，一般情况下是整个ETL的2/3
* ETL常用的三种实现方法：借助ETL工具；SQL方式实现；ETL工具和SQL相结合
* ETL工具解决的问题：数据来自不同物理主机、数据来自不同数据库或者文件、异构数据处理等

### ETL——数据抽取
<img width="753" alt="1a60cf667388c267dace267960c323a" src="https://github.com/user-attachments/assets/c080564e-a57d-47fd-89e3-91433be9b724">

### ETL——数据转换
<img width="773" alt="19be13247428d5047191ad493df4bf8" src="https://github.com/user-attachments/assets/090c02f9-5904-49de-ab97-8d67addcd254">


### ETL——数据加载
<img width="757" alt="6bac7688a3f5d9b3fb7d3809afa1372" src="https://github.com/user-attachments/assets/3b21cf80-1e6b-451b-bc62-46a7f7938d06">

### 常用的ETL工具
* Kettle
* Apatar
* Scriptella
* ETL Automation

## 网络爬虫(互联网数据采集)
* 网络爬虫：是一种按照一定的规则，自动抓取万维网信息（网页）的程序或者脚本。为搜索引擎从万维网上抓取网页，是搜索引擎的重要组成部分
* 网络爬虫可分为通用网络爬虫和聚焦网络爬虫

<img width="758" alt="80bef729b558979181d113d7bf3574c" src="https://github.com/user-attachments/assets/3de3e55a-d4e5-49d2-8f95-2a4cb2233906">

<img width="762" alt="35fca274f6f609a7b8ba4e974b4f7d2" src="https://github.com/user-attachments/assets/ed34165a-3556-468e-aec3-acacbaf8de67">

* 网络爬虫视角下的网页：

<img width="707" alt="d163df8cc73b28de539db47a6d84fde" src="https://github.com/user-attachments/assets/a3e42158-b7af-4149-94cb-64924154ba02">

* 网络爬虫的抓取策略：
  * 深度优先遍历策略：从起始页开始，一个个链接跟踪下去。
  * 宽度优先遍历策略：抓取当前网页中链接的所有网页，再从待抓取队列中选取下一个URL
  * 反向链接数策略：反向链接数是指一个网页被其他网页链接指向的数量。使用这个指标评价网页的重要程度，从而决定抓取先后顺序
  * 基于优先级计算的策略：针对待抓取网页计算优先级值，通过排序来确定抓取顺序。如：Partial PageRank，OPIC等
  * 大站优先策略：对于待抓取队列中的所有网页，根据所属的网站进行分类，对于待下载页面数多的网站优先下载

* 定期更新策略
  * 历史参考策略：在网页的历史更新数据基础上，利用建模等手段，预测网页下一次更新的时间，确定爬取周期
  * 用户体验策略：依据网页多个历史版本的内容更新、搜索质量影响、用户体验等信息，来确定爬取周期
  * 聚类分析策略：首先对海量的网页进行聚类分析，每个类中的网页一般具有类似的更新频率。通过抽样计算，确定针对每个聚类的爬行频率

* 网络爬虫系统架构：往往是一个分布式系统。

<img width="773" alt="cc8249caeddd5e1afea9e8caee013ca" src="https://github.com/user-attachments/assets/60030490-8d48-4866-9141-11e0ed2f6696">

<img width="777" alt="aca30d90fd6b721bc9090518b38982f" src="https://github.com/user-attachments/assets/fce6adb4-db88-4426-b56d-d1dec07187d7">

<img width="802" alt="136c6c96b622180a1ac004082d8d080" src="https://github.com/user-attachments/assets/60f9af3b-0a6d-4176-ba4b-cded91cd1d65">


# 4-数据采集2
## Apache Flume(日志数据采集)
* Flume是分布式、可靠、和高可用的海量日志采集、聚合和传输的日志收集系统。由Cloudera公司开源，连接数据源和数据存储系统的管道，屏蔽数据源和数据存储系统异构性的中间件
* Flume-OG基本架构

<img width="715" alt="b8a2c5565c3f1bbf53f99c8ea11e045" src="https://github.com/user-attachments/assets/67adb078-fe63-4662-a501-c698cc271a5a">

* 基于Flume-OG的美团日志收集系统

<img width="757" alt="4f0a216f5bfe8db62b431fa7554793d" src="https://github.com/user-attachments/assets/ebc0624e-d3ba-4c23-bcd0-486dea13acde">


* Flume-NG基本架构

<img width="774" alt="e0a6b5a0c619a15b8f098ff515b4695" src="https://github.com/user-attachments/assets/a50a5431-6f57-4173-8017-4a4a93722028">

* Flume-NG核心概念
  * Event：Flume数据传输的基本单元，由可选的header和载有数据的byte array构成， byte array可以携带日志数据。
  * Client：将原始日志文件包装成Events并发送它们到一个或多个Agent实体，由独立的线程运行
  * Agent：Flume的运行实体，包含Source, Channel, Sink等组件。利用这些组件将Events从一个节点传输到另一个节点或最终目的地。每台机器运行一个Agent
    * Source：负责接收Event或通过特殊机制产生Event，并将Events批量的放到一个或多个Channel
    * Channel：连接Source和Sink，类似event的缓存队列
    * Sink：接收Event，进行下一步转发

* Flume-OG V.S Flume-NG
  * Flume-OG有三种角色的节点， Flume-NG只有一种角色节点，降低臃肿性，更加灵活
  * 在OG版本中，Flume 的使用稳定性依赖 zookeeper；而在NG版本中，不再需要zookeeper对各类节点进行协调
  * 在Flume-NG中，agent节点的组成也发生了变化，由 source、sink、channel组成

* Flume拓扑结构

<img width="769" alt="a135a6a3718de50c9fc07bba20a2801" src="https://github.com/user-attachments/assets/df5b91ef-9394-477b-93e9-39f10d42dce8">

<img width="782" alt="ca5b9efba1d373ceaa82a77a3215af1" src="https://github.com/user-attachments/assets/c9874cc9-95d4-40dc-a032-428750bf57f9">

<img width="785" alt="3fddca163984cf73933332923560d3e" src="https://github.com/user-attachments/assets/2cf7e3d3-558c-4d41-9699-da0e236cd2d4">

<img width="776" alt="9fe87524407ec188a465bcaf0fa3c0e" src="https://github.com/user-attachments/assets/c4a2b7bd-b700-4514-b310-46a6e992a388">


## Apache Kafka(数据分发中间件)
* 为什么需要数据分发？
  * 前端数据采集后，需要送到后端进行分析处理。前端采集与后端处理往往是多对多的关系。之间需要数据分发中间件负责消息转发，保障消息可靠性，匹配前后端速度差
  * 分布式系统构件之间通过数据分发可以解除相互之间的功能耦合，从而减轻子系统之间的依赖，使得各个子系统或构件可以独立演进、维护或者重用

* 数据分发解决方案——消息队列
  * 消息队列是在消息传输过程中保存消息的容器或中间件，主要目的是提供消息路由并保障消息可靠传递
  * 目前常见的消息队列中间件产品包括：ActiveMQ 、ZeroMQ,RabbitMQ和Kafka
  * 一般消息中间件支持两种模式：消息队列模式及PubSub模式。

* Kafka：分布式发布-订阅消息系统（Pub-Sub模式），最初由LinkedIn公司开发，之后成为Apache项目的一部分。具有极高的消息吞吐量，较强的可扩展性和高可用性，消息传递低延迟，能够对消息队列进行持久化保存，且支持消息传递的"至少送达一次" 语义

* Kafka架构

<img width="763" alt="3501bd7e947da5e8e35c18cff929959" src="https://github.com/user-attachments/assets/cf5d63bc-408d-4277-8412-732ca7d1d9c7">

* 基本概念——Topics & Partition
  * Topics是消息的分类名（或Feed的名称），一个Topic可以认为是一类消息，每个topic将被分成多个partition(区)。partition是以log文件的形式存储在文件系统中，任何发布到partition的消息都会被直接追加到log文件的尾部。Logs文件根据配置要求,保留一定时间后删除来释放磁盘空间。
  * Partition： Topic物理上的分组，一 个 topic可以分为多个partition，每个 partition是一个有序的队列。partition中的每条消息都会被分配一个有序的 id（offset）。
  * Partition设计目的：
    * 通过分区,可以将日志内容分散到多个server上,来避免文件尺寸达到单机磁盘的上限,每个partiton都会被当前server(kafka实例)保存
    * 可以将一个topic切分多任意多个partitions,来提升消息保存/消费的效率
    * 越多的partitions意味着可以容纳更多的consumer，有效提升并发消费的能力

  
<img width="449" alt="9f8f7dae245ec35f989063b274a5a8e" src="https://github.com/user-attachments/assets/511b9e26-3ac7-48a2-afa8-56942905edaa">


* 基本概念——Producer
  * Producer 将消息发布到指定的Topic中,同时Producer 也能决定将此消息归属于哪个partition;比如基于"round-robin"方式或者通过其他的一些算法等
  * 消息和数据生产者，向 Kafka的一个topic发布消息的过程叫做 producers
  * 异步发送：批量发送可以很有效的提高发送效率。Kafka producer的异步发送模式允许进行批量发送，先将消息缓存在内存中，然后一次请求批量发送出去。

* 基本概念——Consumer
  * 消息和数据的消费者，订阅相关topics，并处理Producer发布的消息
  * 允许consumer group（包含多个consumer）对一个topic进行消费，不同的consumer group之间独立订阅
  * 每个consumer属于一个consumer group；发布的消息,只会被订阅此Topic的每个group中的一个consumer消费
  * 同一个group中不能有多于partitions个数的consumer同时消费,否则将意味着某些consumer将无法得到消息。
 
* 基本概念——Broker
  * Broker：缓存代理，Kafka 集群中的一台或多台服务器统称为 broker
  * 为了减少磁盘写入的次数，broker会将消息暂时buffer起来，当消息的个数(或尺寸)达到一定阀值时，再flush到磁盘，这样减少了磁盘IO调用的次数。
  * Broker不保存订阅者的状态，由订阅者自己保存。Broker没有副本机制，一旦broker宕机，该broker的消息将都不可用

* 基本概念——Message
  * Message消息：是通信的基本单位，每个 producer 可以向一个 topic（主题）发布一些消息
  * Kafka中的Message是以topic为基本单位组织的，不同的topic之间是相互独立的。每个topic又可以分成几个不同的partition(每个topic有几个partition是在创建topic时指定的)，每个partition存储一部分Message
  * partition中的每条Message包含了以下三个属性：
    * offset 对应类型：long
    * MessageSize 对应类型：int32
    * data 是message的具体内容

* Kafka的消息发送的流程

<img width="740" alt="280db882d7df1fd5e8ead7a9bf8808f" src="https://github.com/user-attachments/assets/334519ac-f52c-4e15-9e0b-ef52fcd486b2">

* Kafka的消息流示例

<img width="758" alt="9560364fa3ff29d126d7fb3673e6f3b" src="https://github.com/user-attachments/assets/1687b62a-26d9-4a96-990b-ce8a41e00e96">

* Kafka通过消息副本机制提供了高可用的消息服务， 其副本管理单位不是Topic消息队列，而是Topic的数据分片(Partition) 。在配置文件里可以指定数据分片的副本个数
* 在多个副本里，其中一个作为主副本(Leader)，其他作为次级副本(Slave) 。所有针对这个数据分片的消息读/写请求都由主副本来负责响应，次级副本只是以主副本数据消费者的方式从主副本同步数据

* Kafka是一个基于文件系统的消息系统，那么基于磁盘读写操作的消息系统如何保证系统性能？由于磁盘读/写因为涉及寻道操作这种机械运动，所以和内存读/写相比其效率极低。为此，考虑如何运用磁盘本身的特性来极大提升系统效率？
  * 原则：尽可能避免随机读/写，同时尽可能利用顺序读/写， 即连续读/写整块数据,Kafka能够高效处理大批量消息的一个重要原因就是将读/写操作尽可能转换为顺序读/写

## 其他数据采集
* 探针：网络流量数据捕获
* 传感器：环境数据捕获
* RFID Reader：标签数据捕获

# 5-分布式存储
## 大数据存储
* 重要手段：超大规模分布式文件系统，即以文件系统的方式来组织海量数据
* 代表工作：Google的文件系统——GFS

## Google文件系统(GFS )
* GFS 提供了海量非结构化信息的存储平台，并提供数据的冗余备份、成千台服务器的自动负载均衡以及失效服务器检测等各种完备的分布式存储功能
* 在GFS基础功能之上，可以开发更加符合应用需求的存储系统和计算框架

## 文件系统
* 文件系统是操作系统用于明确存储设备或分区上的文件的方法和数据结构，即在存储设备上组织文件的方法

## GFS设计原则 
* GFS采用大量商业PC来构建存储集群，PC的稳定性并没有很高的保障。部件错误不再被当作异常，而是将其作为常见的情况加以处理。数据冗余备份、机器有效性检测、机器故障恢复都被放在设计目标里
* GFS文件系统所存储的文件绝大多数都是大文件（GB），设计时针对大文件的读写操作进行了优化，尽管也支持小文件，但不作为重点
* 系统中存在大量的“追加写” 操作。即将新增内容追加到已有文件末尾，已经写入的内容一般不做更改，很少有文件的"随机写"行为
* 对于数据读取操作来说，绝大多数读文件操作都是“顺序读”，少量操作是“随机读”。往往是一次读取较大量的数据，而不是不断定位到某个位置读取少量数据

## GFS整体架构
* GFS文件系统主要由3个组成部分构成：唯一的主控服务器(Master)、众多的Chunk服务器和GFS客户端
  * 主控服务器：负责整体系统的管理工作
  * Chunk服务器：负责实际的数据存储并响应客户端的读/写请求
  * GFS客户端：用户操作

* GFS由上千台PC构成，但是在应用开发者的眼中，GFS类似于本地的统一文件系统，分布式存储系统的细节是隐藏的
* GFS文件存储：对于开发者而言，GFS类似于Linux文件系统或者是 Windows操作系统提供的文件系统，由目录和存放在目录下的文件构成树形结构，这个树形结构被称为GFS命名空间。GFS向应用开发者提供了文件的创建、删除、读取和写入等常见的操作接口(API)

<img width="422" alt="786ad57f1916cfb78dfe3f54ea13fc7" src="https://github.com/user-attachments/assets/169d9132-3e10-413c-9432-d338c7a74724">

* GFS命名空间由众多的目录和GFS文件构成， 一个GFS文件由众多固定大小的Chunk构成。而每个Chunk又由更小的粒度Block 构成，Chunk是基本存储单元，Block是基本读取单元

<img width="414" alt="ef18ad7a23e6775f9e61affb082d830" src="https://github.com/user-attachments/assets/534a543f-ab5c-45ce-bb0a-3160abac8d73">

## GFS主控服务器 
* GFS主控服务器所管理的系统元数据包括以下三种：
  * GFS命名空间和Chunk命名空间：主要用来对目录文件以及Chunk的增删改等信息进行记录
  * 从文件到其所属Chunk之间的映射关系：因为一个文件会被切割成众多Chunk，所以系统需要维护这种映射关系
  * Chunk在Chunk服务器上的存储信息：每个Chunk会被复制多个备份，并存储在不同的服务器上

<img width="428" alt="7c0f0d7b9cbc8cfa4d05069cce768d8" src="https://github.com/user-attachments/assets/293abf09-9676-4f5c-91c0-31cbe4714d2b">

* 主控服务器上管理数据的安全性需要保障：
  * GFS将前两类管理信息(命名空间，文件到Chunk的映射) 记录在系统日志文件内。并且将系统日志分别存储在多台机器上， 避免信息丢失。
  * 第3类管理信息(Chunk存储信息)通过主控服务器定期询问Chunk服务器来维护的

* 主控服务器所承担的系统管理工作：
  * 不同Chunk服务器之间的负载均衡。创建新的Chunk以及垃圾回收
  * 数据备份及迁移时需要考虑
    * Chunk数据的可用性；当Chunk数据不可用时，及时重新备份
    * 尽可能减少网络传输压力

* GFS是典型的主从结构方式。通过主控服务器实现全局管控，管理便捷
  * 缺点：系统瓶颈，单点失效
* GFS采用增加另外一台影子服务器(Shadow)的方式来解决单点失效问题。当主控服务器出现故障，无法提供服务时。由影子服务器接替主控服务器行使对应的管理功能

## GFS写操作示例
* GFS客户端发出写操作请求，GFS系统必须将这个写操作应用到 Chunk的所有备份，从而维护数据的一致性。对于多个相互备份的Chunk从中选取一个作为主备份，其余为次级备份，主备份决定次级备份的写入顺序

<img width="407" alt="18ef4996ef76aeb3e3987b11221568f" src="https://github.com/user-attachments/assets/f48dd6e1-9b33-4b9f-be21-e298785442a2">


## Colossus：下一代GFS
* Colossus对GFS的改进：
  * GFS主控服务器内存大小决定整个文件系统可以管理文件的数量，扩展性差。Colossus将单一主控服务器改造为多主控服务器构成的集群，将所有管理数据进行数据分片后分配到不同的主控服务器中
  * GFS通过Chunk数据备份来保证数据高可用性，但付出更多存储成本。 Colossus采用纠删码算法，可以在减少备份数目的情形下达到类似的高可用性要求
  * GFS在内部管理Chunk备份存放位置，客户端对此是不可见的，Colossus 增加了客户端的灵活性使得客户端可以管理备份数据的存储地点

## 纠删码(Erasure Code) 
* 为了增加存储系统的可靠性和数据可用性，经常使用数据备份来达到这一点，直接增加存储成本。能否在不对数据做备份的情形下提供类似的数据可用性？
  * 纠删码就是为了达到这一目的而在分布式存储中开始被广泛使用
* 纠删码通过对原始数据进行校验并保留校验数据，以增加冗余的方式来保证数据的可恢复性。
* 基本思想：将数据文件切割为等长的n个数据块，并根据n个数据块生成m个冗余的校验信息，得到n+m块数据。如果其中m块数据丢失，可以通过剩下的n个数据块将其进行恢复。

<img width="419" alt="2a84b39220a670ea26dc991208520bf" src="https://github.com/user-attachments/assets/11cbffc9-30db-40c7-83e3-22f8183dd65e">


## Reed-Solomon编码
* Reed-Solomon编码是最常用的纠删码之一，Google 在Colossus以及Facebook在HDFS-RAID中都已经实现并部署了RS编码来减少存储成本。如果要通过RS编码来计算原始数据的校验信息C，需要设定涉及所有原始数据块的编码函数F

<img width="761" alt="91332f7cceca12c21d429caabb0c39a" src="https://github.com/user-attachments/assets/807b0ed4-e879-49d7-848b-ba30e15bc309">

* RS编码的问题转换：已知n个数据字d1, d2, …, dn分别行储在不同的存储设备上，需要根据这些数据字计算m个校验字c1, c2, …, cm使得n+m个数据字可以容忍最多m个数据损失。
* 要实现这一点，RS编码涉及3 个主要问题：
  * 计算原始数据的校验字
  * 从数据错误中恢复原始数据。
  * 实现快速计算

## LRC编码
* 使用RS编码时，如果将一个文件划分成10个数据块，即使只有其中一个数据块损坏，也需要其他所有数据块来共同恢复这个损坏的数据块。在分布式网络环境下，为了恢复少量数据块，需要大量的网络传输和磁盘IO才能够将其进行恢复
* LRC的提出就是为了解决这一难题，所以LRC面临的问题是：能否在可靠性与RS编码大致相同的情况下，减少恢复损毁数据所需的数据块数量
* Facebook的XorBas系统采用了LRC 来达到这一目标，根据 XorBas的数据，通过采用LRC编码，其恢复损毁数据所需的数据块数量可以降低为RS编码的一半，付出的代价是增加14%的额外存储成本
* LRC编码中存在“块局部性（Block Locality）” 和“最小码距（Minimal Code Distance）”两个概念：
  * 块局部性：指的是对于某个纠删码来说，要对一个数据块编码，最多需要多少其他数据块。恢复某个数据块时需要其他数据块的个数
  * 最小码距：指的是对于切割成n块的文件，最少损毁多少块数据文件就不可恢复了

<img width="760" alt="3e1a5c95048b527c1ce1215761ae394" src="https://github.com/user-attachments/assets/e8a608ca-dd9e-4465-b507-767a5a59b490">

* LRC本质上是在RS编码基础上通过增加数据冗余来换取校验数据的局部性

## Pyramid编码
* 在组内有多个块失效时，LRC编码仍然需要在全局范围内进行修复。为此，可以对数据块进行更 多层次的分组，高层的组包含若干低层较小的组

<img width="747" alt="238d85a5f67e25421cbe409c94d85e8" src="https://github.com/user-attachments/assets/5c7452d2-cb14-4612-986f-f43cb75d881b">


## 层次分组编码
* LRC编码与Pyramid编码均属于层次分组编码
* 思想：将所有数据块分成少数几个较大且互不重叠的组，然后每个组再进一步分成几个若干互不重叠的子组。以此类推，可以根据需要划分多个层次。最后，每组各产生一个只与组内数据块相关的局部校验块，整体再产生若干与所有数据块相关的全局校验块

## 纠删码进行数据容错的挑战 
* 当前，纠删码容错技术在大规模分布式存储中面临的主要挑战已不再是其较高的运算复杂度，而是其较高的网络资源消耗，以及实现上的复杂性
* 纠删码容错技术面临的挑战主要表现在编码实现、数据修复和数据更新３个方面
  * 编码实现：主要挑战在于如何在编码实现完全完成之前保证数据的可靠性
  * 数据修复：挑战主要在于如何降低数据读取量和数据传输量。，从数据修复的具体过程入手，通过优化修复时的数据读取和数据传输过程
  * 数据更新：在基于纠删码容错的数据更新中，需要先将原数据读取出来，重新计算相应的校验数据，然后再写入。数据块关联的校验块较多，需要更新的编码块更多

# 6-分布式计算MapReduce
## MapReduce
* MapReduce分布式计算框架最初是由Google公司于2004年提出的。它不仅仅是一种分布式计算模型，同时也是一整套构建在大规模普通商业PC 之上的批处理计算框架
* 该计算框架可以处理以PB计的数据，并提供了简易应用接口，将系统容错及任务调度等设计分布式计算系统时需考虑的复杂实现很好地封装在内，使得应用开发者只需关注应用逻辑

## 批处理计算
* 批处理是大数据计算中一类常见的计算任务，主要操作大容量静态数据集，并在计算过程完成后返回结果
* 批处理模式中使用的数据集通常具有下列特征：
  * 有界——批处理数据集代表数据的有限集合；
  * 持久——数据通常始终存储在某种类型的持久存储位置中
  * 大量——批处理操作通常是处理极为海量数据集的唯一方法
* 批处理需要访问全套记录才能完成的计算工作。例如计算总数，必须将数据集作为一个整体加以处理。处理需要付出大量时间，不适合对处理时间要求较高的场合。

## MapReduce计算模型
<img width="613" alt="12b25d47ef9bca1f51336bb3d0825f2" src="https://github.com/user-attachments/assets/d7ce93cd-5c99-4ae8-ab2e-bdc15a814557">


## MapReduce执行过程
<img width="625" alt="53656b08a257b80024f0e6130e8548f" src="https://github.com/user-attachments/assets/59f14d17-b565-4a04-aea1-8f46edafdcb4">


## MapReduce计算模式
* 求和模式(Summarization Pattern)
* 过滤模式(Filtering Pattern)
* 数据组织模式(Data Organization Pattern)

## MapReduce数值求和
<img width="647" alt="43232b7d40ab4a2363e6d86a8a1cdd7" src="https://github.com/user-attachments/assets/f2ce48be-ecf0-4730-9df9-50d69c777396">


<img width="623" alt="f15b4a38118993d714a4cbcc689eb62" src="https://github.com/user-attachments/assets/e2dd4a73-914d-46c2-bbd1-ff55e182e48d">

<img width="653" alt="8bfaa5f55cff7e8803795d86be5848d" src="https://github.com/user-attachments/assets/d282b3bf-736d-4026-ab91-3e23a1552465">


## MapReduce中的重要概念
* Combiner可以看做局部的Reducer，它的作用是合并相同的key对应的value。优势是能够减少MapTask输出的数据量（即磁盘IO）。但是并不是所有的场景都可以使用Combiner
* Partitioner决定了Map Task输出的每条数据交给哪个Reduce Task来处理。Partitioner有两个功能：
  * （1）均衡负载。它尽量将工作均匀地分配给不同的Reduce。
  * （2）效率。它的分配速度一定要非常快。Partitioner的默认实现：hash(key)mod R，这里的R代表Reduce Task的数目，意思就是对key进行hash处理然后取模
  
* 从Map输出到Reduce输入的整个过程可以广义地称为Shuffle。Shuffle横跨Map端和Reduce端，在Map端包括Spill过程，在Reduce端包括copy和sort过程，copy是从相应的Map节点中拉取需要的结果到Reduce中计算，一般Reduce是一边copy一边sort。


## 求和模式(Summarization Pattern)
* Mapper以需要统计的数值类型对象ID作为Key，其对应的数值作为Value，如果使用Combiner 会极大地减少Shuffle阶段的网络传输量。另外，Partitioner的设计也很重要，决定交由哪个Reducer来处理，但可能会导致数据分布倾斜(Skewed)。通过Shuffle阶段，MapReduce 将相同对象传递给同一个Reducer，Reducer则对相同对象的若干Value进行数学统计计算， 得到最终结果。

<img width="641" alt="41ec55f6595885e0f8e3dc4b6479c82" src="https://github.com/user-attachments/assets/27650d34-9e8d-4f49-9805-0ebc16773e76">


## 过滤模式(Filtering Pattern)
<img width="647" alt="cd676e1fd89c222c87be6517439339d" src="https://github.com/user-attachments/assets/8f325829-a1fb-4ea9-a288-bd47baeebbc1">


<img width="647" alt="d7251d1aeae0eb2d0d4c80e45eab785" src="https://github.com/user-attachments/assets/197d33a0-059b-4180-a889-a83859dd7dc2">


## 数据组织模式(Data Organization Pattern)
* 数据分片：需要对数据记录进行分类，比如可以将所有记录按照日期进行分类，将同一天的数据放到一起以做后续数据分析。

<img width="338" alt="f84ed1aab6bb7de9b28647f81205f92" src="https://github.com/user-attachments/assets/52945e2f-4017-48a9-8700-3d5827e1b6f7">


* Mapper和Reducer非常简单，其重点在Partitioner策略的设计。通过改变Partition策略，将相同标准的数据经过Shuffle过程放到一起。由同一个Reducer来输出，即可达到按需数据分片的目的


* 全局排序：对海量数据进行全局排序，例如：针对10亿个网页进行单词统计之后，根据出现次数由高到底对所有单词排序
* Mapper 逻辑很简单，只需要将记录中要排序的字段作为Key，记录内容作为Value输出即可。如果设定一个Reducer，那么Reduce过程不需要做额外工作，只需以原样输出即可，因为Reduce过程已经对所有数据进行了全局排序。但如果设定多个Reducer，那么需要在Partition策略上做些研究，因为尽管每个Reducer负责的部分数据是有序的，但是多个Reducer产生了多份部分有序结果，仍然没有得到所需要的全局排序结果。如何解决？
  * 可以通过Partition策略，将数据分发到不同Reducer的时候，保证不同Reducer处理一个范围区间的记录。

## MapReduce实例——文本分析
* 文本中单词统计：给定一个巨大的文本（如1TB），如何计算单词出现的数目？

<img width="638" alt="1a76d5b6e002057770c2040ea7afe69" src="https://github.com/user-attachments/assets/466ff29c-f547-4dac-8a08-317713c990e1">

<img width="619" alt="a23808315e77cff5b703422bc357168" src="https://github.com/user-attachments/assets/2f97ceed-d1ec-42ab-8ae4-0c2c9b9b35d4">

<img width="566" alt="2282e84e282d0cd943edafe7f90c64a" src="https://github.com/user-attachments/assets/78fc3267-3bd8-4948-b73d-c4db6202939f">

<img width="540" alt="f6887f098fec45be1db7d6aebfcfd3e" src="https://github.com/user-attachments/assets/94a9ba58-558f-4ce8-bc2c-539368f6b4fe">


## MapReduce实例——页面点击统计
* 页面点击统计：对于互联网网站来说，通常是通过Log形式将用户行为记载下来。最常见的Log挖掘项目之一是统计页面点击数，即网站各个页面在一定时间段内各自有多少访问量。

<img width="634" alt="206b0e74ee7305758595539f6a9758c" src="https://github.com/user-attachments/assets/0efe7dea-50d2-48ef-99d1-0404ce3ca976">


## MapReduce实例——专利引用统计
* 专利引用统计：一个真实的数据集cite75_99.txt（专利引用数据）,它来自美国国家经济研究局提供的美国专利数据，网址为http://www.nber.org/patents/。部分数据如下：

<img width="651" alt="c8611ae9aedf3b35cd676c201c4b78e" src="https://github.com/user-attachments/assets/e365e766-13b7-4a43-9487-4fd667ef3e18">


## MapReduce计算特点
* 具有极强的可扩展性，可以在数干台机器上并发执行
* 具有很好的容错性，即使集群机器发生故障，一般情况下也不会影响任务的正常执行
* 具有简单性， 用户只需完成Map和Reduce 函数即可完成大规模数据的并行处理


## MapReduce计算缺点
* MapReduce运算机制的优势是数据的高吞吐量、支持海量数据处理的大规模并行处理、细粒度的容错，但是并不适合对时效性要求较高的应用场景，比如交互式查询或者流式计算。也不适合迭代运算类的机器学习及数据挖掘类应用。原因如下：
  * 第一、Map和Reduce任务启动时间较长。因为对于批处理任务来说，其任务启动时间相对后续任务执行时间来说所占比例并不大，所以可忽略不计。但是对于时效性要求高的应用，其启动时间与任务处理时间相比太高。
  * 第二、在一次应用任务执行过程中， MapReduce计算模型存在多处的磁盘读/写及网络传输过程。比如初始的数据块读取、Map任务的中间结果输出、Shuffle阶段网络传输、Reduce阶段的磁盘读及GFS写入等。对于迭代类机器学习应用来说，往往需要一个MapReduce任务反复迭代进行，此时磁盘读/写及网络传输需要反复进行多次，导致任务效率低下。

## 典型迭代算法——K-Means（聚类）
* 聚类分析：根据“物以类聚”原理，将本身尚未归类的样本根据多个维度（多个属性）聚集成不同的组，这样的一组数据对象的集合叫做簇或群组。
* 聚类目标——经过划分后，使得
  * 属于同一群组的样本之间彼此足够相似
  * 属于不同群组的样本应该足够不相似

* 聚类与分类的区别：聚类没有事先预定的类别，不需要人工标注和预先训练分类器，类别在聚类过程中自动生成 。聚类被视为一种无监督的学习
* K均值算法，概括起来有五个步骤：
  * 设定一个数K，表明总共有几个群簇（组）；
  * 从所有实例中随机选择K个实例，分别代表一个群簇的初始中心
  * 对剩余的每个实例，根据其与各个组的初始中心的距离，将它们分配到离自己最近的一个群簇中
  * 然后，更新群簇中心，即：重新计算得出每个群簇的新的中心点
  * 这个过程不断重复（即：重复第3、4步），直到每个群簇中心不再变化，即直到所有实例在K组分布中都找到离自己最近的群簇。或者达到最大迭代次数


## K-Means算法MapReduce化
<img width="648" alt="d751575f7415d5d64e6e1980a71cb94" src="https://github.com/user-attachments/assets/5dca91b9-75ad-4154-9347-1840a8b39e5d">

* 令 k = 2，欲生成 cluster-0 和 cluster-1
* 随机选取A(1,1)作为cluster-0的中心，C(4,3)作为cluster-1的中心
* 假定将所有数据分布到2个节点 node-0 和 node-1 上,即node-0 ：A(1,1)和C(4,3)，node-1 ：B(2,1)和D(5,4)
* 首先创建一个存储聚类中心的全局文件

<img width="628" alt="90fd2768b0a28e88152faa4c9a85228" src="https://github.com/user-attachments/assets/cff343c2-0f01-46a8-ae7b-75373b4128da">


* Map阶段：
  * 每个节点读取全局文件，以获得上一轮迭代生成的cluster centers等信息
  * 计算本节点上的每一个点到各个cluster center的距离
  * 对于每一个数据点，可以得到<cluster assigned to , 数据点自身>

<img width="643" alt="a9390e1d6f511e3852c67b34fb9291e" src="https://github.com/user-attachments/assets/21016a19-aa98-46a2-a901-aefe56476154">


* Combine阶段：
  * 利用combiner减少map阶段产生的大量数据
  * Combiner计算统一簇中所有数据点的均值，以及这些数据点的个数
  * 然后，为每一个cluster发送 key-value pair
    * key - cluster id,
    * value -【 # of data points of this cluster， mean】

<img width="659" alt="6738a91dec9ac44221eded3f3d912f9" src="https://github.com/user-attachments/assets/d9562567-39be-4924-9d48-0483951388e9">


* Reduce阶段：
  * 由于map阶段产生的key是cluster-id，所以每个 cluster的全部数据将被发送同一个reducer，包括：
    * 该cluster 的id
    * 该cluster的数据点的均值，及对应于该均值的数据点的个数
  * 然后经计算后输出
    * 当前的迭代计数
    * cluster id
    * cluster center
    * 属于该cluster center的数据点的个数



<img width="654" alt="fc213c347d6faece9635d88e015d7ec" src="https://github.com/user-attachments/assets/47b76b8c-1be9-412f-a99b-ff803c16bdd8">

<img width="655" alt="274a1cb7d3d28f956c37c4a9b9d510a" src="https://github.com/user-attachments/assets/3e25d30f-7a6c-4ebf-8035-47c299bbbde2">

<img width="632" alt="3114de411ad2f7a3e00c4dbd02c5323" src="https://github.com/user-attachments/assets/8c140878-1001-451d-93b2-f368da99c469">


# 7-Hadoop&spark
## Hadoop简介
* 开源Apache项目，灵感来源于Google的论文
* Hadoop核心组件包括：
  * 分布式文件系统（HDFS）
  * 分布式计算构架（MapReduce）
* 使用Java编写
* 运行平台：Linux

## HDFS：Hadoop分布式文件系统
* HDFS是Hadoop中的大规模分布式文件系统，模仿GFS开发的开源系统。HDFS适合存储大文件并为之提供高吞吐量的顺序读/写访问，不太适合大量随机读的应用场景，也不适合存储大量小文件等应用场景。
* Hadoop 1.x版本中HDFS的整体架构由四部分构成：NameNode 、DataNode 、Secondary NameNode以及客户端。NameNode类似于GFS的主控服务器, DataNode类似于GFS 的Chunk服务器

## HDFS整体架构
<img width="722" alt="6115f327977dac55815df8c6d17b4fe" src="https://github.com/user-attachments/assets/47cff9e6-fe5b-48a5-b9ec-8941865ad0d5">

## HDFS：NameNode 
* NameNode管理整个分布式文件系统的元数据，包括文件目录树结构、文件到数据块Block的映射关系、Block副本及其存储位置等各种管理数据
* 管理数据保存在内存的同时，在磁盘保存两个元数据管理文件：fsimage和editlog。
* NameNode还负责DataNode的状态监控，通过短时间间隔的心跳来传递管理信息和数据信息。通过这种方式的信息传递，NameNode可以获知每个DataNode保存的Block信息、DataNode的健康状况、命令DataNode启动停止等

## HDFS：Secondary NameNode
* Secondary NameNode的职责并非是NameNode的热备机，而是定期从NameNode拉取fsimage和editlog 文件，并对这两个文件进行合并，形成新的fsimage文件并传给NameNode。目的是为了减轻NameNode的工作压力。NameNode本身不做这种合并操作，所以本质上Secondary NameNode是个提供检查点功能服务的服务器

## HDFS：DataNode和客户端
* DataNode负责数据块的实际存储和读写工作，HDFS语境下一般将数据块称为Block而非Chunk。Block默认大小为64MB。客户端上传大文件时，HDFS会自动将其切割成固定大小的Block。为了保证数据可用性，每个Block会以多备份的形式存储，默认备份个数为3
* 客户端和NameNode联系获取所需读写文件的元数据，实际的读写都是和DataNode直接通信完成。其读写流程与GFS读写流程基本一致，不同点在于不支持客户端对同一文件的并发写操作， 同一时刻只能有一个客户端在文件末尾进行追加写操作。

## Hadoop MapReduce示例
<img width="779" alt="c7fb8e4a53bbbd5a4e969bcb15253c2" src="https://github.com/user-attachments/assets/6e596908-d8ad-4c56-af9a-c2e40adeb37a">

## Hadoop 1.0 v.s Hadoop 2.0
<img width="779" alt="3352cd13e9ed1d781a80c79c497d8b0" src="https://github.com/user-attachments/assets/298ee2aa-dd70-498b-b87a-2e15470d0125">

## YARN——资源管理与调度系统
* 资源管理与调度的核心目标：使得整个集群的大量资源在能够实现更高资源利用率的同时加快所有计算任务的整体完成速度
* 当前发展趋势：在集群硬件层之上抽象出一个功能独立的资源管理系统，将所有可用资源当作一个整体来进行管理，并对其他所有计算任务提供统一的资源管理与调度框架和接口
* YARN基本架构

<img width="767" alt="489b72858a126743816f9eaac9b22d7" src="https://github.com/user-attachments/assets/9e6314cb-7f20-4250-8121-8c584638fe1e">

* YARN是个典型的两级调度器。
  * ResourceManager(RM)负责整个集群的资源管理功能，类似于中央调度器
  * 每个任务单独有一个ApplicationMaster(AM)负责完成任务所需资源的申请管理与任务生命周期管理功能，类似于二级调度器，AM负责向RM 申请作业所需资源，并在作业的众多任务中进行资源分配与协调。

* 资源管理器RM部分：
  * 主要负责全局的资源管理工作，其内部主要功能部件包括：调度器、AMService、Client-RM接口以及RM-NM接口。
  * 调度器主要提供各种调度策略，支持可插拔方式，系统管理者可以制定全局的资源分配策略。
  * AMService负责系统内所有AM的启动与运行状态管理。
  * Client-RM接口负责按照一定协议管理客户提交的作业。
  * RM-NM接口主要和各个机器的NM通过心跳方式进行通信，以此来获知各个机器可用的资源以及机器是否产生故障等信息。
 
* 应用服务器AM部分：
  * AM的功能类似于Hadoop 1.0的JobTracker，负责向RM申请启动任务所需的资源，同时协调作业内各个任务的运行过程。
  * AM像普通任务一样运行在某台机器的容器内。RM的AMS负责为作业的AM申请资源并启动它，使得整个作业能够运转起来。之后各种任务管理工作都交由AM 来负责
  * AM作为二级调度器，也负责任务间资源分配时的数据局部性等优化调度策略。

* 节点管理器NM部分：
  * NM是部署在每台机器上的，主要负责机器内的容器资源管理，比如容器间的依赖关系、监控容器执行以及为容器提供资源隔离等各种服务等
  * NM启动以后，向RM进行注册，之后通过心跳方式向RM汇报节点状态并执行RM发送来的命令
  * NM也接收AM发来的命令，比如启动或者杀死某个容器内运行的任务等。


## 资源管理与调度
* 独立资源管理与调度系统的优势：
  * 集群整体资源利用率高，可根据不同计算任务的即时需要动态分配资源
  * 可增加数据共享能力，所有资源对所有任务可用， 只需存储一份即可
  * 支持多类型计算框架和多版本计算框架，不仅可以运行批处理、流式计算、图计算等多种类型特点各异的计算系统，也可以支持多版本计算框架

* 概念模型：主要强调三要素，资源组织模型，调度策略和任务组织模型

<img width="774" alt="2f52e07db9d30edd75cc83192e07d17" src="https://github.com/user-attachments/assets/1772cdc3-dc34-44ff-b109-5471153d0f34">


## Spark简介
* Spark项目于2009年诞生于加州大学伯克利分校AMPLab。是专为大规模数据处理而设计的快速通用的计算框架。在保留了MapReduce优点基础上，实现了基于内存的计算，使得计算时效性有了很大的提高。

## Spark V.S Hadoop
* 处理问题的层面不同：Hadoop被看做是分布式数据基础设施，包括了存储（HDFS）和计算（MapReduce）。Spark是专门用来对大数据进行处理的工具，目标是具备通用性和高效性。自身没有数据存储功能。可处理来自HDFS或其他分布式文件系统的数据。
* Spark数据处理速度远超Hadoop：Hadoop是磁盘级计算，计算时需要在磁盘中读取数据。Spark在内存中以接近“实时”的时间完成所有的数据分析，与Hadoop相比快近100倍。

## 关于Spark内存计算的理解 
* 内存计算不是Spark的特性：Spark只是利用内存来实现数据的缓存，而不是将数据持久化在内存中。Spark允许我们使用内存缓存以及LRU替换规则。
* Spark比Hadoop快的原因：
  * task启动时间比较快，Spark是fork出线程；而MR是启动一个新的进程
  * Spark只有在shuffle的时候才会将数据放在磁盘，而MR却不是
  * 典型的MR工作流是由很多MR作业组成的，他们之间的数据交互需要把数据持久化到磁盘才可以；而Spark支持DAG以及pipelining，在没有遇到shuffle完全可以不把数据缓存到磁盘。
  * 虽然目前HDFS也支持缓存，但是一般来说，Spark的缓存功能更加高效，特别是在SparkSQL中，我们可以将数据以列式的形式储存在内存中

## Spark特性
<img width="770" alt="a42fa4942065a1dc57fe697e29b5619" src="https://github.com/user-attachments/assets/f30ad7ff-bac1-4883-910a-a08580037ebc">


## Spark生态系统
<img width="785" alt="553c2c6799b457b4ac9843e74570753" src="https://github.com/user-attachments/assets/1e65caba-abec-4b6c-8b6a-beb524aa2212">

## Spark计算核心层
<img width="770" alt="d77cd7d323a209080870484587b171e" src="https://github.com/user-attachments/assets/a47d9db9-f7b6-48c1-8db8-69aa0aaa7e0b">

## Spark程序基本流程
<img width="739" alt="21c5ebe03a55c86cb876e51743cb2e3" src="https://github.com/user-attachments/assets/d79598b8-f605-47fb-87e0-e7e7d423407e">

## 什么是DAG
* DAG（Directed Acyclic Graph）有向无环图的简称。DAG计算模型往往是指将计算任务在内部分解成为若干个子任务。这些子任务之间由逻辑关系或运行先后顺序等因素被构建成有向无环图结构。
* 大数据处理领域的DAG计算系统一般包含三层架构：
  * 应用表达层（最上层）：通过一定手段将计算任务分解成由若干个子任务形成的DAG结构。这层的核心是表达的便捷性，主要目的是方便应用开发者快速描述或者构建应用
  * DAG执行引擎层（中间层）：主要目的是将上层以特殊方式表达的DAG计算任务通过转换和映射。将其部署到下层的物理机集群中来真正运行。是DAG计算系统的核心部件。
  * 物理机集群（最下层）：由大量物理机搭建的分布式计算环境，计算任务最终执行的场所。

## 什么是RDD
* RDD：弹性分布式数据集（resilient distributed dataset）是Spark的核心数据模型，RDD是Spark中待处理的数据的抽象，它是逻辑中的实体
* 在对RDD进行处理的时候不需要考虑底层的分布式集群，就像在单机上一样即可。
* RDD是只读的分区记录集合。只能基于稳定物理存储中的数据集或在已有的RDD上执行确定性操作来创建
* RDD可以被认为是提供了一种高度限制的共享内存（只读，只能从稳定的存储或已有的RDD创建而来。）

* RDD示例：在系统日志文件中找出HDFS出错的时间

<img width="654" alt="45cecd32caa8e681a4faabf7db93734" src="https://github.com/user-attachments/assets/6edd6a35-671b-4ecb-9d9b-c85c56ca1a96">

## RDD特点
* RDD表示已被分区，不可变的并能够被并行操作的数据集合。可以从程序数据集，HDFS数据，流数据等不同数据集格式来构建相应的RDD
* RDD是可序列化的。RDD也可以cache到内存中
* 在创建RDD时Spark会维护RDD衍生及转换的信息。需要时候，可以从物理存储的数据计算出最终的 RDD
* RDD有两种计算方式：转换（Transformation，返回值还是一个RDD）与操作（Action，返回值不是RDD）

<img width="782" alt="20b9ba8787659d67311f84ff81799cd" src="https://github.com/user-attachments/assets/5382da9f-005e-47a7-a58e-e027aca27008">

## RDD转换与操作与操作函数
<img width="771" alt="38706e4c214c35f07c663cde2c85397" src="https://github.com/user-attachments/assets/56cf80b7-71ec-465d-beb4-ef8764284591">

<img width="753" alt="4214f9584743fd6819607369db23094" src="https://github.com/user-attachments/assets/2d0a8052-7c6e-40c0-ae41-7ecd8e0f63e7">

<img width="772" alt="01f537467c1fdea462e82c9b3b79534" src="https://github.com/user-attachments/assets/9cf0f95b-ff3a-41b9-a6c7-8e92dd45d1b7">

## RDD“血统” 
* 一个作业从开始到结束的计算过程中产生了多个 RDD，RDD之间是彼此相互依赖的，我们把这种父子依赖的关系称之为“血统”。RDD的变换序列均由“血统” 存储下来，为容错提供便利
* RDD的容错机制又称“血统”容错。 要实现这种容错机制，最大的难题就是如何表达父 RDD 和子 RDD 之间的依赖关系
* RDD中的依赖划分成了两种类型：窄依赖 (Narrow Dependencies) 和宽依赖 (Wide Dependencies)

## RDD依赖
* 窄依赖 (Narrow Dependencies) ：是指父RDD的每个分区都只被子RDD的一个分区所使用
* 宽依赖(Wide Dependencies)：就是指父RDD的分区被多个子 RDD 的分区所依赖。

<img width="756" alt="08923ebb217f8cf928bb87833118e8c" src="https://github.com/user-attachments/assets/055584f6-713a-47fc-88bd-ace27b46f299">

## Spark RDD运行原理
* RDD在Spark架构中的运行主要分为三步：
  * 创建 RDD 对象
  * DAGScheduler模块介入运算，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG
  * 每一个JOB被分为多个Stage，划分Stage的一个主要依据是RDD之间的依赖关系，从后往前推，遇到宽依赖就断开，划分为一个stage

<img width="743" alt="32422054b56ec6020922c7a2dec3cd2" src="https://github.com/user-attachments/assets/2e71937e-1d0c-4b5f-8be7-2980dae7e9d0">

<img width="781" alt="4dcad107a663ca8bb525738ef91946f" src="https://github.com/user-attachments/assets/8ed4ca40-21e2-4a26-8671-cbb9b774377f">

## Spark技术堆栈/Spark MLlib架构解析/Spark的高可用性
* Spark使用Spark RDD、 Spark SQL、 Spark Streaming、 MLlib、 GraphX成功解决了大数据领域中，离线批处理、 交互式查询、 实时流计算、 机器学习与图计算等最重要问题
* Spark MLlib 是Spark中可以扩展的机器学习库，它有一系列的机器学习算法和实用程序组成。包括分类、回归、聚类、协同过滤等，还包含一些底层优化方法
* Spark Mlib架构解析

<img width="777" alt="5790cbd73c77c56552d960075124f7f" src="https://github.com/user-attachments/assets/3d388606-5c8d-448a-b6d3-e105ff621545">

* Spark的高可用性

<img width="732" alt="363824d568e8cbfdc73485e24be2cba" src="https://github.com/user-attachments/assets/013dcdea-f065-48bc-98c3-68edc8c6c1f6">

# 8-流式计算
## 批处理计算
* 批处理是大数据计算中一类常见的计算任务，主要操作大容量静态数据集，并在计算过程完成后返回结果
* 批处理模式中使用的数据集通常具有下列特征：
  * 有界——批处理数据集代表数据的有限集合；
  * 持久——数据通常始终存储在某种类型的持久存储位置中；
  * 大量——批处理操作通常是处理极为海量数据集的唯一方法

* 批处理需要访问全套记录才能完成的计算工作。例如计算总数，必须将数据集作为一个整体加以处理。处理需要付出大量时间，不适合对处理时间要求较高的场合

## 流式计算
* 流处理系统会对随时进入系统的数据进行计算，无需针对整个数据集执行操作
* 流式计算特点：
  * 流处理中的数据集是“无边界”的，数据集只能代表截至目前已经进入到系统中的数据总量。
  * 流处理系统可以处理几乎无限量的数据，但同一时间只能处理一条（真正的流处理）或很少量（微批处理，Microbatch Processing）数据。
  * 处理工作是基于事件的，除非明确停止否则没有“尽头”。处理结果立刻可用，并会随着新数据的抵达继续更新


* 有近实时处理需求的任务很适合使用流处理模式。
* 流式计算(Stream Processing)是越来越受到重视的一个计算领域。在很多应用场所，对大数据处理的计算时效性要求很高，要求计算能够在非常短的时延(Low Latency)内完成，这样能够更好地发挥流式计算系统的威力。

## 流式计算一般框架
* 流式计算一般框架可以分为三部分：
  * 数据实时采集
    * --Apache Flume：日志，事件等数据资源的收集系统
    * ---Kafka：一个可持久化的分布式的消息队列（自带存储）
  * 数据实时计算（流式计算系统）
    * ---Storm：Apache顶级项目，基于DAG计算模型
    * ---S4: Yahoo的分布式流计算平台，P2P结构
    * ---Samza：Apache开源项目，将流作为消息处理
    * ---Spark Streaming：Apache Spark子项目
  * 流式计算应用
    * ---Druid：大数据实时查询和分析系统
    * ---Splunk：机器数据（系统，集群等数据）分析引擎

## 流式计算系统
* 当前的流式计算系统可称为“可扩展数据流平台类”的计算系统。其设计初衷是出于模仿MapReduce计算框架的思路，即在对处理时效性有高要求的计算场景下，如何提供一个完善的计算框架，并暴露给用户少量的编程接口。使得用户能够集中精力处理应用逻辑。至于系统性能、低延迟、数据不丢失以及容错等问题，则由计算框架来负责。

## 流式计算系统的特点
* 相比批处理计算系统，流式计算系统具备特点：
  * 记录处理低延迟：从原始输入数据进入流式系统， 再流经各个计算节点后抵达系统输出端。整个计算过程所经历的时间越短越好，主流的流式计算系统对于记录的处理时间应该在毫秒级
  * 极佳的系统容错性：保证数据不会丢失、保证数据的送达，以及对计算状态的持久化、快速的计算迁移和故障恢复等都是必需的要求
  * 极强的系统扩展能力：在系统满足高可扩展的同时，不能因为系统规模增大而明显降低流式计算系统的处理速度。
  * 灵活强大的应用逻辑表达能力：应用逻辑描述的灵活性，操作原语的多样性与灵活性。

## 流式计算系统架构
* 主从模式：主控节点做全局管理比较简洁，比如Storm和Samza都是这类架构
* P2P模式：避免单点失效，但无中心控制节点，系统管理方面相对较复杂。如Yahoo的S4

## 流式计算系统：Storm
* Storm：一个开源框架，Apache顶级项目，来自Twitter公司，其目标是大数据流的实时处理。可以可靠地处理无限的数据流。
* 分布式
  * 水平扩展：通过加机器、提高并发数就提高处理能力
  * 自动容错：自动处理进程、机器、网络异常
* 实时：数据不写磁盘，延迟低（毫秒级）
* 流式：不断有数据流入、处理、流出
* 开源：twitter开源，社区很活跃

## Storm计算模型——DAG
<img width="767" alt="80c8e608cff13f4699ea322d94e21b8" src="https://github.com/user-attachments/assets/01fd1f71-3791-49e2-86bc-bc46431caa14">


## Storm计算模型——Stream
* Storm对于流Stream的抽象：流是一个不间断的无界的连续Tuple（元组，是元素有序列表）。
* Stream消息流，是一个没有边界的Tuple序列，这些Tuples会被以一种分布式的方式并行地创建和处理。

## Storm计算模型——Spout
* Storm认为每个Stream都有一个源头，该源头被抽象为Spouts
* Spout：流数据源，它会从外部读取流数据，并发出Tuple。也可以看出是从外部获取数据，同时输出原始的Tuple

## Storm计算模型——Bolt
* Storm将流的中间状态转换抽象为Bolts，Bolts可以处理Tuples，同时它也可以发送新的流给其他Bolts使用。
* Bolts：消息处理者，所有的消息处理逻辑被封装在Bolts里面，处理输入的数据流并产生输出的新数据流，可执行过滤，聚合，查询数据库等操作
* 接收 Spout/Bolt 输出的Tuple，处理，输出新Tuple。

## Storm计算模型——Task，Stream Groupings
<img width="764" alt="24ffce87f6b974f857b291402b46742" src="https://github.com/user-attachments/assets/fc9595cf-8b1f-4408-a5e1-afca83c8c304">

## Storm计算模型——Topology
<img width="771" alt="e1bc68ecc967569665ef7377d2f56db" src="https://github.com/user-attachments/assets/91a29fd0-4a9c-4776-a384-c96132ba5ae2">

## Storm体系架构——主从式
* 系统存在两类节点:主控节点和工作节点

<img width="752" alt="dd0261fe4eeb17a9755ac87f8a1a4c6" src="https://github.com/user-attachments/assets/c0b31691-85a9-452f-bec1-65b6854b6c05">

## Storm体系架构设计特点：
* Nimbus后台程序和Supervisor后台程序都是快速失败（fail-fast）和无状态的，所有状态维持在Zookeeper或本地磁盘。
* 这种设计中Nimbus并没有直接和Supervisor通信， 而是借助中介Zookeeper ， 这样可以分离Nimbus和Supervisor的依赖，将状态信息存放在zookeeper集群内以快速回复任何失败的一方。
* 这意味着你可以杀掉Nimbus进程和Supervisor进程，然后重启，它们将恢复状态并继续工作，这种设计使Storm极其稳定

## Storm系统执行单元：
* Worker：工作进程；拓扑以一个或多个Worker进程的方式运行。每个Worker进程是一个物理的Java虚拟机，执行拓扑的一部分任务。在Storm集群中，Worker是由Supervisor来生成，一台主机对应一个Supervisor，可生成一个或多个Worker。
* Executor：执行线程；1个worker进程会启动1个或多个executor线程来执行task。1个executor线程对应topology中一个组件（spout或bolt）的task。

<img width="542" alt="f9ae78c84e51bd98fe2fd4fb8fb9ebe" src="https://github.com/user-attachments/assets/f36e5401-1511-42a3-8bd0-8bf8828c8256">

## Storm工作流程/代码片段解析：
<img width="719" alt="d7b620c18578f3d7c4fb44bf60bb7a7" src="https://github.com/user-attachments/assets/fd7c9100-8e57-4e1e-82e3-d721c83cbf03">

<img width="776" alt="2168f29757d611b403747ccb1f3be0b" src="https://github.com/user-attachments/assets/f1761444-cee1-4f05-b0d6-7646f1b15518">


## 流式计算系统：Spark Streaming
* BDAS中的重要构成部分
* Spark Streaming是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。
* Spark Streaming支持从多种数据源获取数据，包括Kafka、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets
* 从数据源获取数据之后，可以使用诸如map、reduce、join等高级函数进行复杂算法的处理
* 最后还可以将处理结果存储到文件系统，数据库

* Spark Streaming工作机制：
  * Spark Streaming是将流式计算分解成一系列短小的批处理作业（mini-batch）。这里的批处理引擎是Spark Core。也就是把输入数据按照batch size分成一段一段的数据（Discretized Stream），每一段数据都会转换成Spark中的RDD
  * 然后将Spark Streaming中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。
  * 整个流式计算根据业务的需求可以对中间的结果进行叠加或者存储到外部设备。
  * 这是一个典型的生产者消费者模型。需要考虑如何协调生产速率和消费速率

* Spark Streaming架构：

<img width="624" alt="ce29850ab904ebcbb29b652fec3a4be" src="https://github.com/user-attachments/assets/86866a58-bfb4-4b25-8722-881d3ad1ea09">

# 9-数据分析算法.md
## 机器学习 VS 数据挖掘
* 从时间轴来看
  * 统计学——18世纪40年代
  * 人工智能—20世纪40-50年代
  * 机器学习—20世纪50年代
  * 数据挖掘—20世纪80-90年代

* 从侧重点来看
  * 机器学习：偏向理论，强调算法及模型（有监督、无监督、半监督、深度学习、强化学习等）
  * 数据挖掘：更偏向于应用， 使用了机器学习技术，预测 、关联分析、聚类等




## 数据挖掘
* 数据挖掘：从大量的数据中通过算法搜索隐藏于其中信息的过程， 是数据库知识发现中的一个步骤。通常与计算机科学有关 ，并通   过统计、在线分析处理、情报检索、机器学习、专家系统（依靠   过去的经验法则）和模式识别等诸多方法来实现上述目标。
* 数据挖掘是建立在其他学科相互影响相互融合之上的新方法。 它   通过一种系统和协同的方式整合了不同学科的知识 ，包括统计学、人工智能、机器学习、管理科学、信息系统以及数据库等。


<img width="753" alt="83e3afbdc506083a0400ef796a2f9e1" src="https://github.com/user-attachments/assets/15a9e07b-7364-4130-a01f-0add0d8a5988">


## 传统数据挖掘思路
* 数据挖掘前奏
* 数据预处理
* 挖掘算法设计
* 结果验证
* 模型反复改进

## 数据挖掘前奏——了解数据类型
* 多种数据的类型
  * 文本、序列、图片、视频– 特征抽取
  * 属性数据
  * 图、树结构数据– 数据的关联关系和数据内容


## 数据挖掘前奏——掌握数据集特点
* 数据集的特点
  * – 数据的稀疏性
  * – 数据的分布
  * – 数据的覆盖范围

## 数据挖掘前奏——数据相似性度量

<img width="698" alt="d07bd9c4d8a9c205b42cceffa919f36" src="https://github.com/user-attachments/assets/62622511-e59e-4073-b938-1339b064ac71">


## 数据预处理——意义
* 现实世界的中“脏数据”较多
  * 不完整
  * 有噪声
  * 数据不一致

* 没有高质量的数据，就没有高质量的挖掘结果
  * 高质量的决策必须依赖高质量的数据
  * 数据仓库需要对高质量的数据进行一致地集成

* 数据预处理将是构建数据仓库或者进行数据挖 掘的工作中占工作量最大的一个步骤

## 数据预处理——主要任务
* 数据清理:填写空缺的值，平滑噪声数据，识别、删除孤立点，解决不 一致性
* 数据集成:集成多个数据库、数据立方体或文件
* 数据变换:规范化和聚集
* 数据归约:得到数据集的压缩表示，它小得多，但可以得到相同或相近的结果
* 数据离散化:数据归约的一部分，通过概念分层和数据的离散化来规约数据，对数字型数据特别重要

## 数据预处理——数据清理
* 数据清理任务
  * 填写空缺的值
  * 识别离群点和平滑噪声数据
  * 纠正不一致的数据
  * 解决数据集成造成的冗余

## 数据预处理——数据集成
* 数据集成：将多个数据源中的数据整合到一个一致的存储中
* 模式集成：集合不同数据源中的元数据
* 实体识别问题：匹配来自不同数据源的现实世界的实体
* 检测并解决数据值的冲突：对现实世界中的同一实体，来自不同数据源的属性值可能是不同的

## 数据预处理——数据变换
* 数据变换将数据转换或统一成适合挖掘的形式
  * 平滑：去除数据中的噪声
  * 聚集：汇总，数据立方体的构建
  * 数据泛化：沿概念分层向上汇总
  * 规范化：将数据按比例缩放，使之落入一个小的特定区间
    *  最小－最大规范化
    *   z-score规范化
    *   小数定标规范化

* 属性构造： 通过现有属性构造新的属性，并添加到属性集中；以增加对高维数据的结构的理解和精确度

## 数据预处理——数据归约
* 为什么需要进行数据归约？
  * 数据仓库中往往存有海量数据，在整个数据集上进行 复杂的数据分析与挖掘需要很长的时间

* 数据归约
  * 数据归约可以用来得到数据集的归约表示，它小得多 ,但可以产生相同的（或几乎相同的）分析结果

* 常用的数据归约策略
  * 维归约， e.g. 移除不重要的属性
  * 数据压缩
  * 离散化和概念分层产生

## 数据预处理——离散化
* 离散化
  *  将连续属性的范围划分为区间
  *  有些分类算法只接受离散属性值
  *  通过离散化有效的规约数据
  *  离散化的数值用于进一步分析

* 概念分层
  * 通过使用高层的概念（比如：青年、中年、老年）来 替代底层的属性值（比如：实际的年龄数据值）来规约数据

## 大数据分析 VS 传统数据挖掘
* 数据采集主要解决了数据从哪里来的问题
* 特征工程一般是指最大限度地从所收集到的原始 数据中提取特征供后续算法使用
* 数据挖掘一般是指揭示大数据背后隐藏的信息。
* 可视化分析是关于数据视觉表现形式的技术。

## 特征工程
* 特征工程被描述为一种过程。
* 在数据基础上利用领域知识来构建适用于机器学习算法的特征。
* 需要明白数据和特征的区别与联系。

* 特征工程的三个层次：
  * 原始信息的获取与转换
    * 用什么样的传感器获取信号
    * 对从传感器中得到的信号，可称之为原始信息
  * 特征的提取
    * 特征提取方法是充分发挥设计者智慧的过程
    * 这个层次的工作往往因事物而易，与设计者本人的知识结构也有关
    * 这个层次的工作是最关键的，但太缺乏共性
  * 特征空间的优化
    * 这个层次的工作发生在已有了特征的描述方法之后，也就是已有了一个初始的特征空间 ，如何对它进行改造与优化的问题
    * 所谓优化一般是要求既降低特征的维数，又能提高分类器的性能

## 特征工程-实例
* 文本数据的特征提取一般是通过建立向量空间模型 为后续的机器学习算法提供量化信息作为输入
* 向量空间模型一般也被称为词袋模型(Bag  of Words)。该模型采用关键词作为项， 将关键词出现的频率或在频率统计基础上计算的权重作为每一项对应的值， 由此构成特征向量

## 总结：特征工程主要内容
* 特征处理：
  * 特征清洗：清洗异常样本 ，特征采样。
  * 特征预处理：
    * 单个特征——归一化 ， 离散化 ，缺失值等
    * 多个特征——降维 ，特征选择等
* 特征监控：特征有效性分析等

## 数据挖掘基本算法
<img width="763" alt="b4df14cafaf0b75f8bdaabcd58e729c" src="https://github.com/user-attachments/assets/e6f6c0dc-fc48-41d6-87a5-258a7c62ad18">



## 分类算法
* 分类：在一群已经知道类别标号的样本中，训练一种分类器，让其能够对某种未知的样本进行类别预测
* 分类算法属于一种有监督的学习。分类过程就是建立 一种分类模型来描述给定的数据集或概念集
* 分类的目的就是使用分类器对新的数据集进行划分， 其主要涉及分类规则的准确性、过拟合、矛盾划分的取舍等。


## 分类算法——贝叶斯分类器
<img width="737" alt="d1dd9337fce59d4e886f07b6262814a" src="https://github.com/user-attachments/assets/9b554404-f215-4505-90f9-cc392235c228">

<img width="740" alt="ac1b45aa61af4a6f31d6f7fbb5ce7e8" src="https://github.com/user-attachments/assets/cd71739d-0929-48a1-bfc5-46e86c0278bc">

<img width="757" alt="c0bed86b920f460ddc239091713e95e" src="https://github.com/user-attachments/assets/1276cc8d-e541-4b43-a586-17080600f576">

<img width="768" alt="8ce3e48d04916139ba1c385d9bf7c6c" src="https://github.com/user-attachments/assets/56862922-86bf-4daa-9001-cc4e80b3b8a5">

<img width="744" alt="e7f61ec1a7e79daf462892f7766a316" src="https://github.com/user-attachments/assets/e1da0f87-ce47-405d-b3a4-47878ce63863">


## 分类算法——K最近邻（KNN)
* 基本思想是 ，针对待分类数据 ，从训练集中找K个最近的邻居， 由这些邻居投票决定待分类数据的类别。
* 最近邻分类是基于要求的或懒散的学习法，即它存放所有的 训练样本，并且直到新的（未 标记的）样本需要分类时才建立分类。其优点是可以生成任意形状的决策边界，能提供更加灵活的模型表示。
* 跟其他分类算法不同：
  * 其他算法都是先根据预分类的训练集来训练模型，然后抛开训练集进行预测
  * 而KNN的训练集就是模型本身

* 要求训练集中各个分类的数量要体现实际当中这些类别出现的概率
* 面临诸多挑战
  * 维度灾难
  * 存储开销
  * 查询速度

## 分类算法——决策树
* 什么是决策树？
  * 类似于流程图的树结构
  * 每个内部节点表示在一个属性上的测试
  * 每个分枝代表一个测试输出
  * 每个树叶节点代表类或类分布

* 决策树的生成由两个阶段组成
  * 决策树构建
    * 开始时，所有的训练样本都在根节点
    * 递归的通过选定的属性，来划分样本（必须是离散值）
  * 树剪枝
    * 许多分枝反映的是训练数据中的噪声和孤立点，树剪枝试图检测和剪去这种分枝

* 决策树的使用：对未知样本进行分类– 通过将样本的属性值与决策树相比较

## 决策树构建基本思想
* 基本算法 (贪婪算法)
  * 自顶向下的分治算法构造树
  * 开始, 所有的训练样本和树根相连
  * 属性为分类属性 (若是连续值，则离散化)
  * 根据选定的属性递归地划分样本?如何选择
    * 基于启发式或统计度量选取测试属性 (e.g., 信息增益)

* 停止划分的准则
  * 所有样本均和属于同一类的节点连接
  * 无剩下的属性用于继续划分样本 –叶节点分类应用多数表决法
  * 无剩余的样本
  * 其它的提前中止法

* 属性选择度量－划分规则
  * 划分属性：度量得分高的属性

* 流行的属性选择度量
  * 信息增益(ID3 ， C4.5) - 选取时，偏向于多值属性
  * 增益率(C4.5) - 偏向不平衡划分
  * Gini指标( CART, SLIQ, SPRINT) - 偏向于多值属性, 类的数量很大时，计算较困难

## 决策树代表性算法——ID3
* ID3算法是以信息论为基础的， 该算法通过计算信息 熵和信息增益作为选择特征的衡量标准 ，每次选择信息增益最大的特征来对训练样本集进行划分， 直至划分结束

<img width="690" alt="a4206fa78e7da32fb942d2bf47ee212" src="https://github.com/user-attachments/assets/65509d44-c9d3-4181-900d-0253c95b4eb3">

## 关联分析
* 关联分析（关联规则）：用于发现隐藏在大型数 据集中的令人感兴趣的联系，所发现的模式通常用关联规则或频繁项集的形式表示。

## 关联分析——相关定义
* 项集（Itemset）
  * 包含0个或多个项的集合
  * k-项集(如果一个项集包含k个项)

* 支持度计数（Support count ） (σ)
  * 包含特定项集的事务个数
  *  σ ({Milk, Bread, Diaper}) = 2

* 支持度（Support）
  * 包含项集的事务数与总事务数的比值
  * 例如：   s({Milk, Bread, Diaper}) = 2/5

* 频繁项集（Frequent Itemset）
  *  满足最小支持度阈值的所有项集

<img width="320" alt="b59473614d9df4dbd002e1ce107027a" src="https://github.com/user-attachments/assets/71867982-05a4-4750-b425-fcfeaf38b3a8">

<img width="729" alt="443e3e1ebab3d1f07ff7f7f4bcf25f0" src="https://github.com/user-attachments/assets/48004adf-f090-42ce-91f4-f76b19a4a82e">

## 关联分析——关联规则挖掘
<img width="763" alt="df856e1398f05734d9d35ab8c933f2a" src="https://github.com/user-attachments/assets/3863c446-b46a-4990-a029-197e781d73d5">

<img width="740" alt="af3fcab3405924731d1064022d91ef4" src="https://github.com/user-attachments/assets/d8668848-ec0d-4446-95f3-afc724f7409c">

* 利用先验(apriori)原理减少候选项集的数量
* 先验原理: 如果一个项集是频繁的，则它的所有子集一 定也是频繁；如果一个项集是非频繁的，则它的所有超集也一定是非频繁的
  * 这种基于支持度度量修剪指数搜索空间的策略称为基于支持度的剪枝（support-based pruning）
  * 这种剪枝策略依赖于支持度度量的一个关键性质，即一个项集的支持度决不会超过它的子集的支持度。这个性质也称为支持度度量的反单调性（anti-monotone）。

* Apriori算法
  * 大多数关联规则挖掘算法通常采用的一种策略是，将关联规则挖掘任务分解为如下两个主要的子任务:
    * 频繁项集产生（Frequent Itemset Generation）:其目标是发现满足最小支持度阈值的所有项集，这些项集称作频 繁项集。 (支持度表示本身出现的频繁性)
    * 规则的产生（Rule Generation）:其目标是从上一步发现的频繁项集中提取所有高置信度的规则， 这些规则称作强规则（strong rule）。（置信度表示关联性）

<img width="737" alt="d5c164113301ed8a57a7232250a68ca" src="https://github.com/user-attachments/assets/9527cbb7-045d-4ba3-8785-bdca9c0694cb">


## 聚类分析——分簇状态挖掘
* 聚类分析：
  * 根据“物以类聚 ”原理 ，将本身尚未归类的样本根据多个维度 （多个属性） 聚集成不同的组 ，这样的一组数据对象的集合叫 做簇或群组
  * 聚类目标——经过划分后 ，使得：
    * 属于同一群组的样本之间彼此足够相似
    * 属于不同群组的样本应该足够不相似
  * 聚类与分类的区别： 聚类没有事先预定的类别 ，不需要人工标注和预先训练分类器 ，类别在聚类过程中自动生成 。聚类被视为一种无监督的学习

* 聚类算法分类：
  * 划分方法（Partitioning Method）：包括K均值方法等
  * 次方法（Hierarchical Method）
  * 基于密度的方法（ Density-based ）
  * 基于网格的方法（ Grid-based ）
  * 基于模型的方法（ Model-based ）

* 聚类算法——K均值（K-means）算法
  * 使用最多的聚类算法
  * 它是划分方法的一种。
  * 它原理简单，容易实现。
  * 它适合使用数值型属性，而不是类别型属性。
  * 它的一个不足之处是：对于离两个群组的中心都 很近的点，你会不知道该放到哪个群组中。这其实也是其他一些聚类算法的局限性。

* K均值算法，概括起来有五个步骤：
  * 设定一个数K，表明总共有几个群簇（组）；
  * 从所有实例中随机选择K个实例，分别代表一个群簇的初始中心；
  * 对剩余的每个实例，根据其与各个组的初始中心的距离，将 它们分配到离自己最近的一个群簇中；
  * 然后，更新群簇中心，即：重新计算得出每个群簇的新的中心点；
  * 这个过程不断重复（即：重复第3、4步），直到每个群簇中心不再变化，即直到所有实例在K组分布中都找到离自己最近的群簇。或者达到最大迭代次数。

* 聚类分析示例：

<img width="755" alt="0e7cc5a6e7fc9be9d039cadbb585d39" src="https://github.com/user-attachments/assets/b5d73148-72ef-4677-a533-31ce550ddea3">

<img width="741" alt="de86f0af9b33221976518c4bfd49c5b" src="https://github.com/user-attachments/assets/a31117f5-708e-4101-8d9b-155d031fa577">

<img width="719" alt="8f25797556ac68b7e27d37c090bad01" src="https://github.com/user-attachments/assets/e8d016af-a895-4bed-abd1-d20bc60a1bdf">

<img width="701" alt="597a07ccef9c7833970c6fb287a85c1" src="https://github.com/user-attachments/assets/85b77af1-9c1f-4c4a-b3c0-d4564287cdc2">

<img width="752" alt="6bd87b5d3c933cdeabf8634b268d2aa" src="https://github.com/user-attachments/assets/4c37a6cc-e294-4d7e-93c9-0349dead390c">

<img width="718" alt="4607029d89001a64f755a2d8ddf72e2" src="https://github.com/user-attachments/assets/a9968048-4032-4428-bec7-a6697ce0af47">

* 聚类分析其他应用：
  * 离群点检测和聚类是高度相关的。
  * 聚类是发现数据集中的主要群体，而离群点检测则试图识别 那些显著偏离多数实例的异常情况
  * 离群点检测可以用聚类方法，但也可以用其它方法，例如： 分类方法
  * 其常见的目的是：信用卡欺诈检测。基于聚类的图像压缩。

# 10-数据可视化
## 数据可视化
* 数据可视化是关于数据的视觉表现形式的研究；其中，这种数据的视觉表现形式被定义为一种以某种概要形式抽提出来的信息，包括相应信息单位的各种属性和变量。
* 数据可视化起源于图形学、计算机图形学、人工智能、科学可视化以及用户界面等领域的相互促进和发展，是当前计算机科学的一个重要研究方向，它利用计算机对抽象信息进行直观的表示，以利于快速检索信息和增强认知能力。
* 可视化设计原则
  * 数据的筛选
  * 数据的可视化的直观映射——充分利用人们已有的先验知识，从而降低人们对信息的感知和认知所需要的时间
  * 视图选择与交互
  * 可视化中的美学因素
  * 可视化隐喻——将数据特性与自然界真实物体结合起来，通过读者对自然界物体的认知来增强可视化表达效果

<img width="622" alt="0e6f0525c02a1905e7565adaaf7026c" src="https://github.com/user-attachments/assets/91197f4c-24f0-4ce7-920d-ea331c105fae">

<img width="601" alt="ae9e74e5fd2b3bda16e226a5520d0e9" src="https://github.com/user-attachments/assets/4b582c8e-febc-4376-9b94-2f3083c6bd05">

* 数据可视化实现工具

<img width="654" alt="4146087ebb66e6a32a6e293f860cebd" src="https://github.com/user-attachments/assets/7284cd72-9e46-43bd-b2de-2fa711593c22">

* R语言是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。
* D3.js 是一个基于数据操作文档JavaScript库。D3帮助你给数据带来活力通过使用HTML、SVG和CSS。D3重视Web标准为你提供现代浏览器的全部功能，而不是给你一个专有的框架。结合强大的可视化组件和数据驱动方式Dom操作。
* 可展开的数据可视化练习-Matplotlib
