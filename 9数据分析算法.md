# 9-数据分析算法.md
## 机器学习 VS 数据挖掘
* 从时间轴来看
  * 统计学——18世纪40年代
  * 人工智能—20世纪40-50年代
  * 机器学习—20世纪50年代
  * 数据挖掘—20世纪80-90年代

* 从侧重点来看
  * 机器学习：偏向理论，强调算法及模型（有监督、无监督、半监督、深度学习、强化学习等）
  * 数据挖掘：更偏向于应用， 使用了机器学习技术，预测 、关联分析、聚类等




## 数据挖掘
* 数据挖掘：从大量的数据中通过算法搜索隐藏于其中信息的过程， 是数据库知识发现中的一个步骤。通常与计算机科学有关 ，并通   过统计、在线分析处理、情报检索、机器学习、专家系统（依靠   过去的经验法则）和模式识别等诸多方法来实现上述目标。
* 数据挖掘是建立在其他学科相互影响相互融合之上的新方法。 它   通过一种系统和协同的方式整合了不同学科的知识 ，包括统计学、人工智能、机器学习、管理科学、信息系统以及数据库等。


<img width="753" alt="83e3afbdc506083a0400ef796a2f9e1" src="https://github.com/user-attachments/assets/15a9e07b-7364-4130-a01f-0add0d8a5988">


## 传统数据挖掘思路
* 数据挖掘前奏
* 数据预处理
* 挖掘算法设计
* 结果验证
* 模型反复改进

## 数据挖掘前奏——了解数据类型
* 多种数据的类型
  * 文本、序列、图片、视频– 特征抽取
  * 属性数据
  * 图、树结构数据– 数据的关联关系和数据内容


## 数据挖掘前奏——掌握数据集特点
* 数据集的特点
  * – 数据的稀疏性
  * – 数据的分布
  * – 数据的覆盖范围

## 数据挖掘前奏——数据相似性度量

<img width="698" alt="d07bd9c4d8a9c205b42cceffa919f36" src="https://github.com/user-attachments/assets/62622511-e59e-4073-b938-1339b064ac71">


## 数据预处理——意义
* 现实世界的中“脏数据”较多
  * 不完整
  * 有噪声
  * 数据不一致

* 没有高质量的数据，就没有高质量的挖掘结果
  * 高质量的决策必须依赖高质量的数据
  * 数据仓库需要对高质量的数据进行一致地集成

* 数据预处理将是构建数据仓库或者进行数据挖 掘的工作中占工作量最大的一个步骤

## 数据预处理——主要任务
* 数据清理:填写空缺的值，平滑噪声数据，识别、删除孤立点，解决不 一致性
* 数据集成:集成多个数据库、数据立方体或文件
* 数据变换:规范化和聚集
* 数据归约:得到数据集的压缩表示，它小得多，但可以得到相同或相近的结果
* 数据离散化:数据归约的一部分，通过概念分层和数据的离散化来规约数据，对数字型数据特别重要

## 数据预处理——数据清理
* 数据清理任务
  * 填写空缺的值
  * 识别离群点和平滑噪声数据
  * 纠正不一致的数据
  * 解决数据集成造成的冗余

## 数据预处理——数据集成
* 数据集成：将多个数据源中的数据整合到一个一致的存储中
* 模式集成：集合不同数据源中的元数据
* 实体识别问题：匹配来自不同数据源的现实世界的实体
* 检测并解决数据值的冲突：对现实世界中的同一实体，来自不同数据源的属性值可能是不同的

## 数据预处理——数据变换
* 数据变换将数据转换或统一成适合挖掘的形式
  * 平滑：去除数据中的噪声
  * 聚集：汇总，数据立方体的构建
  * 数据泛化：沿概念分层向上汇总
  * 规范化：将数据按比例缩放，使之落入一个小的特定区间
    *  最小－最大规范化
    *   z-score规范化
    *   小数定标规范化

* 属性构造： 通过现有属性构造新的属性，并添加到属性集中；以增加对高维数据的结构的理解和精确度

## 数据预处理——数据归约
* 为什么需要进行数据归约？
  * 数据仓库中往往存有海量数据，在整个数据集上进行 复杂的数据分析与挖掘需要很长的时间

* 数据归约
  * 数据归约可以用来得到数据集的归约表示，它小得多 ,但可以产生相同的（或几乎相同的）分析结果

* 常用的数据归约策略
  * 维归约， e.g. 移除不重要的属性
  * 数据压缩
  * 离散化和概念分层产生

## 数据预处理——离散化
* 离散化
  *  将连续属性的范围划分为区间
  *  有些分类算法只接受离散属性值
  *  通过离散化有效的规约数据
  *  离散化的数值用于进一步分析

* 概念分层
  * 通过使用高层的概念（比如：青年、中年、老年）来 替代底层的属性值（比如：实际的年龄数据值）来规约数据

## 大数据分析 VS 传统数据挖掘
* 数据采集主要解决了数据从哪里来的问题
* 特征工程一般是指最大限度地从所收集到的原始 数据中提取特征供后续算法使用
* 数据挖掘一般是指揭示大数据背后隐藏的信息。
* 可视化分析是关于数据视觉表现形式的技术。

## 特征工程
* 特征工程被描述为一种过程。
* 在数据基础上利用领域知识来构建适用于机器学习算法的特征。
* 需要明白数据和特征的区别与联系。

* 特征工程的三个层次：
  * 原始信息的获取与转换
    * 用什么样的传感器获取信号
    * 对从传感器中得到的信号，可称之为原始信息
  * 特征的提取
    * 特征提取方法是充分发挥设计者智慧的过程
    * 这个层次的工作往往因事物而易，与设计者本人的知识结构也有关
    * 这个层次的工作是最关键的，但太缺乏共性
  * 特征空间的优化
    * 这个层次的工作发生在已有了特征的描述方法之后，也就是已有了一个初始的特征空间 ，如何对它进行改造与优化的问题
    * 所谓优化一般是要求既降低特征的维数，又能提高分类器的性能

## 特征工程-实例
* 文本数据的特征提取一般是通过建立向量空间模型 为后续的机器学习算法提供量化信息作为输入
* 向量空间模型一般也被称为词袋模型(Bag  of Words)。该模型采用关键词作为项， 将关键词出现的频率或在频率统计基础上计算的权重作为每一项对应的值， 由此构成特征向量

## 总结：特征工程主要内容
* 特征处理：
  * 特征清洗：清洗异常样本 ，特征采样。
  * 特征预处理：
    * 单个特征——归一化 ， 离散化 ，缺失值等
    * 多个特征——降维 ，特征选择等
* 特征监控：特征有效性分析等

## 数据挖掘基本算法
<img width="763" alt="b4df14cafaf0b75f8bdaabcd58e729c" src="https://github.com/user-attachments/assets/e6f6c0dc-fc48-41d6-87a5-258a7c62ad18">



## 分类算法
* 分类：在一群已经知道类别标号的样本中，训练一种分类器，让其能够对某种未知的样本进行类别预测
* 分类算法属于一种有监督的学习。分类过程就是建立 一种分类模型来描述给定的数据集或概念集
* 分类的目的就是使用分类器对新的数据集进行划分， 其主要涉及分类规则的准确性、过拟合、矛盾划分的取舍等。


## 分类算法——贝叶斯分类器
<img width="737" alt="d1dd9337fce59d4e886f07b6262814a" src="https://github.com/user-attachments/assets/9b554404-f215-4505-90f9-cc392235c228">

<img width="740" alt="ac1b45aa61af4a6f31d6f7fbb5ce7e8" src="https://github.com/user-attachments/assets/cd71739d-0929-48a1-bfc5-46e86c0278bc">

<img width="757" alt="c0bed86b920f460ddc239091713e95e" src="https://github.com/user-attachments/assets/1276cc8d-e541-4b43-a586-17080600f576">

<img width="768" alt="8ce3e48d04916139ba1c385d9bf7c6c" src="https://github.com/user-attachments/assets/56862922-86bf-4daa-9001-cc4e80b3b8a5">

<img width="744" alt="e7f61ec1a7e79daf462892f7766a316" src="https://github.com/user-attachments/assets/e1da0f87-ce47-405d-b3a4-47878ce63863">


## 分类算法——K最近邻（KNN)
* 基本思想是 ，针对待分类数据 ，从训练集中找K个最近的邻居， 由这些邻居投票决定待分类数据的类别。
* 最近邻分类是基于要求的或懒散的学习法，即它存放所有的 训练样本，并且直到新的（未 标记的）样本需要分类时才建立分类。其优点是可以生成任意形状的决策边界，能提供更加灵活的模型表示。
* 跟其他分类算法不同：
  * 其他算法都是先根据预分类的训练集来训练模型，然后抛开训练集进行预测
  * 而KNN的训练集就是模型本身

* 要求训练集中各个分类的数量要体现实际当中这些类别出现的概率
* 面临诸多挑战
  * 维度灾难
  * 存储开销
  * 查询速度

## 分类算法——决策树
* 什么是决策树？
  * 类似于流程图的树结构
  * 每个内部节点表示在一个属性上的测试
  * 每个分枝代表一个测试输出
  * 每个树叶节点代表类或类分布

* 决策树的生成由两个阶段组成
  * 决策树构建
    * 开始时，所有的训练样本都在根节点
    * 递归的通过选定的属性，来划分样本（必须是离散值）
  * 树剪枝
    * 许多分枝反映的是训练数据中的噪声和孤立点，树剪枝试图检测和剪去这种分枝

* 决策树的使用：对未知样本进行分类– 通过将样本的属性值与决策树相比较

## 决策树构建基本思想
* 基本算法 (贪婪算法)
  * 自顶向下的分治算法构造树
  * 开始, 所有的训练样本和树根相连
  * 属性为分类属性 (若是连续值，则离散化)
  * 根据选定的属性递归地划分样本?如何选择
    * 基于启发式或统计度量选取测试属性 (e.g., 信息增益)

* 停止划分的准则
  * 所有样本均和属于同一类的节点连接
  * 无剩下的属性用于继续划分样本 –叶节点分类应用多数表决法
  * 无剩余的样本
  * 其它的提前中止法

* 属性选择度量－划分规则
  * 划分属性：度量得分高的属性

* 流行的属性选择度量
  * 信息增益(ID3 ， C4.5) - 选取时，偏向于多值属性
  * 增益率(C4.5) - 偏向不平衡划分
  * Gini指标( CART, SLIQ, SPRINT) - 偏向于多值属性, 类的数量很大时，计算较困难

## 决策树代表性算法——ID3
* ID3算法是以信息论为基础的， 该算法通过计算信息 熵和信息增益作为选择特征的衡量标准 ，每次选择信息增益最大的特征来对训练样本集进行划分， 直至划分结束

<img width="690" alt="a4206fa78e7da32fb942d2bf47ee212" src="https://github.com/user-attachments/assets/65509d44-c9d3-4181-900d-0253c95b4eb3">

## 关联分析
* 关联分析（关联规则）：用于发现隐藏在大型数 据集中的令人感兴趣的联系，所发现的模式通常用关联规则或频繁项集的形式表示。

## 关联分析——相关定义
* 项集（Itemset）
  * 包含0个或多个项的集合
  * k-项集(如果一个项集包含k个项)

* 支持度计数（Support count ） (σ)
  * 包含特定项集的事务个数
  *  σ ({Milk, Bread, Diaper}) = 2

* 支持度（Support）
  * 包含项集的事务数与总事务数的比值
  * 例如：   s({Milk, Bread, Diaper}) = 2/5

* 频繁项集（Frequent Itemset）
  *  满足最小支持度阈值的所有项集

<img width="320" alt="b59473614d9df4dbd002e1ce107027a" src="https://github.com/user-attachments/assets/71867982-05a4-4750-b425-fcfeaf38b3a8">

<img width="729" alt="443e3e1ebab3d1f07ff7f7f4bcf25f0" src="https://github.com/user-attachments/assets/48004adf-f090-42ce-91f4-f76b19a4a82e">

## 关联分析——关联规则挖掘
<img width="763" alt="df856e1398f05734d9d35ab8c933f2a" src="https://github.com/user-attachments/assets/3863c446-b46a-4990-a029-197e781d73d5">

<img width="740" alt="af3fcab3405924731d1064022d91ef4" src="https://github.com/user-attachments/assets/d8668848-ec0d-4446-95f3-afc724f7409c">

* 利用先验(apriori)原理减少候选项集的数量
* 先验原理: 如果一个项集是频繁的，则它的所有子集一 定也是频繁；如果一个项集是非频繁的，则它的所有超集也一定是非频繁的
  * 这种基于支持度度量修剪指数搜索空间的策略称为基于支持度的剪枝（support-based pruning）
  * 这种剪枝策略依赖于支持度度量的一个关键性质，即一个项集的支持度决不会超过它的子集的支持度。这个性质也称为支持度度量的反单调性（anti-monotone）。
